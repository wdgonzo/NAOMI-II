{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q--DfzctBdz"
   },
   "source": [
    "# NAOMI-II: Semantic-Driven Transparent Dimension Training\n",
    "\n",
    "**Revolutionary approach: Dimensions emerge through semantic clustering!**\n",
    "\n",
    "Uses your existing pre-parsed Wikipedia + WordNet data:\n",
    "- **197K words** (WordNet + Wikipedia vocabulary)\n",
    "- **1.79M triples** (1.56M parse + 230K WordNet)\n",
    "- **971 antonym pairs** for semantic axis discovery\n",
    "- **100K Wikipedia sentences** (already parsed)\n",
    "\n",
    "## Key Innovation\n",
    "\n",
    "**Old approach (WRONG):**\n",
    "- Force 15 \"polarity dimensions\"\n",
    "- Train for opposition as goal\n",
    "- Sparsity as penalty\n",
    "\n",
    "**New approach (RIGHT):**\n",
    "- ALL dimensions = semantic axes (morality, temperature, size, etc.)\n",
    "- Words positioned by meaning\n",
    "- Opposition emerges from opposite meanings\n",
    "- Sparsity emerges from semantic irrelevance\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Semantic-driven embeddings:**\n",
    "- `embeddings`: Position on each semantic axis\n",
    "- `relevance`: Which axes matter for this word's meaning\n",
    "- Final embedding = embeddings × relevance (automatic semantic sparsity!)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "\"good\" on morality axis: value=+0.8, relevance=1.0 → +0.8 ✓\n",
    "\"good\" on temperature: value=0.1, relevance=0.0 → 0.0 ✓ (semantically correct!)\n",
    "```\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- 10-30 semantic axes emerge naturally\n",
    "- Each axis = one global concept (morality, size, temperature, politics, quality, etc.)\n",
    "- Words activate 5-20 axes on average (semantic sparsity)\n",
    "- Antonyms naturally oppose on shared relevant axes\n",
    "- Clear, interpretable dimensional structure\n",
    "\n",
    "**Runtime:** ~10-12 hours on A100 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Upload to Google Drive at `/NAOMI-II-data/wikipedia_100k_graph/`:\n",
    "- `vocabulary.json` (197K words)\n",
    "- `triples.pkl` (1.79M triples)\n",
    "- `training_examples.pkl` (1.66M examples)\n",
    "- `graph_stats.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmH0dPwbtBd3"
   },
   "source": [
    "## Step 1: Setup and Load Pre-Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQ6Nsd_otBd3",
    "outputId": "bc5f4542-6ee8-4133-9206-c18545654a2c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "✓ GPU: NVIDIA A100-SXM4-80GB (85.2 GB)\n",
      "✓ Device: cuda\n",
      "✓ Data path: /content/drive/MyDrive/NAOMI-II-data/wikipedia_100k_graph\n",
      "✓ Results path: /content/drive/MyDrive/NAOMI-II-results/semantic_dims\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy nltk tqdm scikit-learn matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"/content/drive/MyDrive/NAOMI-II-data/wikipedia_100k_graph\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/NAOMI-II-results/semantic_dims\"\n",
    "CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"✓ GPU: {gpu_name} ({vram_gb:.1f} GB)\")\n",
    "    if 'A100' not in gpu_name:\n",
    "        print(f\"⚠️  WARNING: Not A100! Training will be slower.\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU detected!\")\n",
    "\n",
    "print(f\"✓ Device: {device}\")\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Results path: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWiaC4S0tBd5",
    "outputId": "894ba009-19cf-41bf-e631-be08d7a57e3b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "LOADING PRE-PARSED WIKIPEDIA + WORDNET DATA\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading vocabulary...\n",
      "  ✓ Vocabulary: 197,095 words\n",
      "\n",
      "[2/4] Loading triples...\n",
      "  ✓ Triples: 1,785,340\n",
      "\n",
      "[3/4] Loading training examples...\n",
      "  ✓ Training examples: 1,662,260\n",
      "\n",
      "[4/4] Extracting antonym pairs...\n",
      "  ✓ Antonym pairs: 971\n",
      "\n",
      "======================================================================\n",
      "DATA LOADED SUCCESSFULLY\n",
      "======================================================================\n",
      "  Vocabulary: 197,095 sense-tagged words\n",
      "  Triples: 1,785,340\n",
      "  Training examples: 1,662,260\n",
      "  Source sentences: 100,000\n",
      "  Antonym pairs: 971 ← KEY FOR SEMANTIC AXES!\n",
      "======================================================================\n",
      "\n",
      "Sample antonym pairs:\n",
      "  comparably ↔ incomparably\n",
      "  leeward ↔ windward\n",
      "  noblewoman ↔ nobleman\n",
      "  cash ↔ credit\n",
      "  diapsid ↔ anapsid\n",
      "  hardware ↔ software\n",
      "  ascent ↔ descent\n",
      "  complexity ↔ simplicity\n",
      "  intelligence ↔ stupidity\n",
      "  king ↔ queen\n"
     ]
    }
   ],
   "source": [
    "# Load pre-parsed data from Google Drive\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING PRE-PARSED WIKIPEDIA + WORDNET DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load vocabulary\n",
    "print(\"\\n[1/4] Loading vocabulary...\")\n",
    "with open(f\"{DATA_PATH}/vocabulary.json\", 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "    word_to_id = vocab_data['word_to_id']\n",
    "    vocabulary = list(word_to_id.keys())\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "print(f\"  ✓ Vocabulary: {len(vocabulary):,} words\")\n",
    "\n",
    "# Load triples\n",
    "print(\"\\n[2/4] Loading triples...\")\n",
    "with open(f\"{DATA_PATH}/triples.pkl\", 'rb') as f:\n",
    "    triples = pickle.load(f)\n",
    "\n",
    "print(f\"  ✓ Triples: {len(triples):,}\")\n",
    "\n",
    "# Load training examples\n",
    "print(\"\\n[3/4] Loading training examples...\")\n",
    "with open(f\"{DATA_PATH}/training_examples.pkl\", 'rb') as f:\n",
    "    training_examples = pickle.load(f)\n",
    "\n",
    "print(f\"  ✓ Training examples: {len(training_examples):,}\")\n",
    "\n",
    "# Extract antonym pairs\n",
    "print(\"\\n[4/4] Extracting antonym pairs...\")\n",
    "antonym_pairs = []\n",
    "for triple in triples:\n",
    "    source_word, relation, target_word = triple\n",
    "    if 'antonym' in relation.lower():\n",
    "        antonym_pairs.append((source_word, target_word))\n",
    "\n",
    "print(f\"  ✓ Antonym pairs: {len(antonym_pairs):,}\")\n",
    "\n",
    "# Load stats\n",
    "with open(f\"{DATA_PATH}/graph_stats.json\", 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Vocabulary: {stats['vocabulary_size']:,} sense-tagged words\")\n",
    "print(f\"  Triples: {stats['num_triples']:,}\")\n",
    "print(f\"  Training examples: {stats['num_training_examples']:,}\")\n",
    "print(f\"  Source sentences: {stats['num_sentences']:,}\")\n",
    "print(f\"  Antonym pairs: {len(antonym_pairs):,} ← KEY FOR SEMANTIC AXES!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample antonyms\n",
    "print(\"\\nSample antonym pairs:\")\n",
    "for word1, word2 in antonym_pairs[:10]:\n",
    "    w1_display = word1.split('_wn.')[0] if '_wn.' in word1 else word1\n",
    "    w2_display = word2.split('_wn.')[0] if '_wn.' in word2 else word2\n",
    "    print(f\"  {w1_display} ↔ {w2_display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al81NItstBd6"
   },
   "source": [
    "## Step 2: Initialize Semantic-Driven Model\n",
    "\n",
    "**Key innovation:** Each word learns which semantic axes are relevant to its meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx2u0LD6tBd7"
   },
   "outputs": [],
   "source": [
    "    'relevance_commitment_weight': 5.0,  # Increased from 0.5 - stronger commitment!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-9xNbN_tBd8"
   },
   "source": [
    "## Step 3: Semantic-Driven Loss Functions\n",
    "\n",
    "No forced polarity structure - dimensions emerge through semantic clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTZVggxDtBd9"
   },
   "outputs": [],
   "source": [
    "# Match antonym pairs to IDs (with fuzzy matching)print(\"Matching antonym pairs to vocabulary...\")antonym_indices = []for word1, word2 in antonym_pairs:    idx1 = word_to_id.get(word1)    idx2 = word_to_id.get(word2)        # Fuzzy match if needed    if idx1 is None:        for w in word_to_id:            if w.startswith(word1.split('_wn')[0] + \"_wn.\"):                idx1 = word_to_id[w]                break    if idx2 is None:        for w in word_to_id:            if w.startswith(word2.split('_wn')[0] + \"_wn.\"):                idx2 = word_to_id[w]                break        if idx1 is not None and idx2 is not None:        antonym_indices.append((idx1, idx2))antonym_tensor = torch.tensor(antonym_indices, dtype=torch.long, device=device)print(f\"✓ Matched {len(antonym_indices):,} / {len(antonym_pairs):,} antonym pairs\\n\")# Loss functionsdef distance_loss(model, edge_samples):    \"\"\"Semantic distance loss with parse/WordNet weighting.\"\"\"    if len(edge_samples) == 0:        return torch.tensor(0.0, device=device)        parse_samples = []    wordnet_samples = []        for source_id, target_id, target_dist, relation in edge_samples:        if relation.startswith('RelationType.'):            parse_samples.append((source_id, target_id, target_dist))        else:            wordnet_samples.append((source_id, target_id, target_dist))        total_loss = torch.tensor(0.0, device=device)        if parse_samples:        source_ids = torch.tensor([s for s, t, d in parse_samples], device=device)        target_ids = torch.tensor([t for s, t, d in parse_samples], device=device)        target_dists = torch.tensor([d for s, t, d in parse_samples], device=device, dtype=torch.float32)        source_emb = model.get_masked_embeddings(source_ids)        target_emb = model.get_masked_embeddings(target_ids)        actual_dists = torch.norm(source_emb - target_emb, dim=1)        parse_loss = F.mse_loss(actual_dists, target_dists)        total_loss += CONFIG['parse_weight'] * parse_loss        if wordnet_samples:        source_ids = torch.tensor([s for s, t, d in wordnet_samples], device=device)        target_ids = torch.tensor([t for s, t, d in wordnet_samples], device=device)        target_dists = torch.tensor([d for s, t, d in wordnet_samples], device=device, dtype=torch.float32)        source_emb = model.get_masked_embeddings(source_ids)        target_emb = model.get_masked_embeddings(target_ids)        actual_dists = torch.norm(source_emb - target_emb, dim=1)        wordnet_loss = F.mse_loss(actual_dists, target_dists)        total_loss += CONFIG['wordnet_weight'] * wordnet_loss        return total_lossdef semantic_clustering_loss(model, antonym_tensor):    \"\"\"    Encourage semantically-related antonym pairs to use same dimensions.        Mechanism:    1. Compute pair centroids (semantic similarity)    2. Compute dimensional signatures (which dims they use)    3. Force signature similarity to match semantic similarity        Result: (good/bad) and (right/wrong) both use \"morality\" dimension!    \"\"\"    if len(antonym_tensor) == 0:        return torch.tensor(0.0, device=device)        # Get masked embeddings (respects relevance)    emb1 = model.get_masked_embeddings(antonym_tensor[:, 0])    emb2 = model.get_masked_embeddings(antonym_tensor[:, 1])        # Semantic signatures (what concepts are these pairs about?)    pair_centroids = (emb1 + emb2) / 2    semantic_sim = torch.mm(F.normalize(pair_centroids, dim=1),                           F.normalize(pair_centroids, dim=1).T)        # Dimensional signatures (which dimensions do they use?)    diff_vectors = emb1 - emb2    dimensional_sim = torch.mm(F.normalize(diff_vectors, dim=1),                              F.normalize(diff_vectors, dim=1).T)        # Force dimensional usage to match semantic similarity    # Similar pairs → use same dimensions    # Different pairs → use different dimensions    return F.mse_loss(dimensional_sim, semantic_sim)def relevance_coherence_loss(model, antonym_tensor):    \"\"\"    Antonym pairs should activate the same dimensions.        If 'good' activates morality dimension, 'bad' should too    (even though they have opposite values).    \"\"\"    if len(antonym_tensor) == 0:        return torch.tensor(0.0, device=device)        rel1 = torch.sigmoid(model.relevance_logits[antonym_tensor[:, 0]])    rel2 = torch.sigmoid(model.relevance_logits[antonym_tensor[:, 1]])        # Cosine similarity of relevance patterns    relevance_similarity = F.cosine_similarity(rel1, rel2, dim=1)        # Maximize similarity (both activate same axes)    return -torch.mean(relevance_similarity)def relevance_sparsity_loss(model, target_dims):    \"\"\"    Target-based sparsity: aim for specific number of dimensions per word.        Don't just minimize activation - target the semantic sweet spot!    Target: 10 dimensions per word on average.        Uses SOFT counting (sum of sigmoid values) for differentiability.    \"\"\"    relevance = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])        # Average number of active dimensions per word (soft count)    # Sum sigmoid values across dimensions for each word, then average    avg_active_dims = torch.mean(torch.sum(relevance, dim=1))        # Penalize deviation from target    # If avg = 10 and target = 10, loss = 0 (perfect!)    # If avg = 102 and target = 10, loss = high (too dense!)    # If avg = 0 and target = 10, loss = high (too sparse!)    target_tensor = torch.tensor(target_dims, device=avg_active_dims.device, dtype=avg_active_dims.dtype)    return F.mse_loss(avg_active_dims, target_tensor)def relevance_commitment_loss(model):    \"\"\"    Force dimensions to commit to being active (1.0) or inactive (0.0).        Uses entropy penalty: sigmoid values near 0.5 have HIGH entropy (uncertain).    We want LOW entropy (committed).        Entropy of Bernoulli(p) = -p*log(p) - (1-p)*log(1-p)    - At p=0.5: entropy = 0.69 (maximum uncertainty)    - At p=0.0 or p=1.0: entropy = 0 (fully committed)    \"\"\"    relevance = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])        # Compute entropy for each dimension (higher = less committed)    # Add epsilon to avoid log(0)    eps = 1e-8    entropy = -(relevance * torch.log(relevance + eps) +                 (1 - relevance) * torch.log(1 - relevance + eps))        # Penalize high entropy (force commitment)    return torch.mean(entropy)def regularization_loss(embeddings, num_anchors):    \"\"\"L2 regularization on learned dimensions.\"\"\"    return torch.mean(embeddings[:, num_anchors:] ** 2)print(\"✓ Semantic-driven loss functions initialized\")print(\"  - Semantic clustering: Pairs with similar meanings use same dimensions\")print(\"  - Relevance coherence: Antonyms activate same dimensions\")print(\"  - Relevance sparsity: TARGET-BASED (aim for 10 dims/word, not minimize!)\")print(\"  - Uses SOFT counting for differentiable gradients\")print(\"  - Relevance commitment: FORCES binary decisions (0 or 1, not 0.5!)\")print(\"  - Natural emergence of semantic axes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1ovUWWytBd-"
   },
   "source": [
    "## Step 4: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKtSsIkftBd_"
   },
   "outputs": [],
   "source": [
    "class KnowledgeGraphDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_id, relation, target_id = self.examples[idx]\n",
    "        target_distance = 0.5\n",
    "        if 'synonym' in relation.lower():\n",
    "            target_distance = 0.1\n",
    "        elif 'antonym' in relation.lower():\n",
    "            target_distance = 0.9\n",
    "        return source_id, target_id, target_distance, relation\n",
    "\n",
    "# Split dataset\n",
    "random.seed(42)\n",
    "random.shuffle(training_examples)\n",
    "\n",
    "split_idx = int(0.9 * len(training_examples))\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "train_dataset = KnowledgeGraphDataset(train_examples)\n",
    "val_dataset = KnowledgeGraphDataset(val_examples)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=12, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "print(f\"✓ Train examples: {len(train_examples):,}\")\n",
    "print(f\"✓ Val examples: {len(val_examples):,}\")\n",
    "print(f\"✓ Train batches: {len(train_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0GRCZ_ntBeA"
   },
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Dimensions emerge through semantic clustering - no forced structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukGMReDGtBeB"
   },
   "outputs": [],
   "source": [
    "# Optimizeroptimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])scaler = GradScaler('cuda') if CONFIG['mixed_precision'] else None# Historyhistory = {'train_loss': [], 'val_loss': [], 'avg_relevance_soft': [], 'avg_relevance_hard': []}best_val_loss = float('inf')patience_counter = 0def compute_avg_relevance(model):    \"\"\"    Compute average dimensions per word.    Returns both soft count (what the loss sees) and hard count (>0.5 threshold).    \"\"\"    rel = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])        # Soft count: sum of sigmoid values (matches loss function)    soft_count = torch.mean(torch.sum(rel, dim=1)).item()        # Hard count: number of dimensions > 0.5 (for interpretability)    hard_count = torch.mean(torch.sum(rel > 0.5, dim=1).float()).item()        return soft_count, hard_countprint(\"=\"*70)print(\"STARTING SEMANTIC-DRIVEN TRAINING\")print(\"=\"*70)print(\"Key difference: Dimensions emerge through semantic clustering\")print(\"No forced polarity structure - natural semantic axes!\")print(f\"Target: {CONFIG['target_dims_per_word']:.0f} dimensions per word (soft count)\")print(\"=\"*70)print()start_time = time.time()for epoch in range(1, CONFIG['epochs'] + 1):    # Train    model.train()    total_loss = 0.0        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")    for source_ids, target_ids, target_dists, relations in pbar:        edge_samples = list(zip(source_ids.tolist(), target_ids.tolist(), target_dists.tolist(), relations))        optimizer.zero_grad()                if CONFIG['mixed_precision']:            with autocast('cuda'):                d_loss = distance_loss(model, edge_samples)                sc_loss = semantic_clustering_loss(model, antonym_tensor)                rc_loss = relevance_coherence_loss(model, antonym_tensor)                rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])                commit_loss = relevance_commitment_loss(model)                r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])                                total = (                    d_loss +                    CONFIG['semantic_clustering_weight'] * sc_loss +                    CONFIG['relevance_coherence_weight'] * rc_loss +                    CONFIG['relevance_sparsity_weight'] * rs_loss +                    CONFIG['relevance_commitment_weight'] * commit_loss +                    CONFIG['reg_weight'] * r_loss                )            scaler.scale(total).backward()            scaler.step(optimizer)            scaler.update()        else:            d_loss = distance_loss(model, edge_samples)            sc_loss = semantic_clustering_loss(model, antonym_tensor)            rc_loss = relevance_coherence_loss(model, antonym_tensor)            rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])            commit_loss = relevance_commitment_loss(model)            r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])                        total = (                d_loss +                CONFIG['semantic_clustering_weight'] * sc_loss +                CONFIG['relevance_coherence_weight'] * rc_loss +                CONFIG['relevance_sparsity_weight'] * rs_loss +                CONFIG['relevance_commitment_weight'] * commit_loss +                CONFIG['reg_weight'] * r_loss            )            total.backward()            optimizer.step()                # Preserve anchors        with torch.no_grad():            if model.embeddings.grad is not None:                model.embeddings.grad[:, :CONFIG['num_anchors']] = 0.0            if model.relevance_logits.grad is not None:                model.relevance_logits.grad[:, :CONFIG['num_anchors']] = 0.0                total_loss += total.item()        pbar.set_postfix({'loss': f\"{total.item():.4f}\", 'sc': f\"{sc_loss.item():.4f}\"})        # Validate    model.eval()    val_loss = 0.0    with torch.no_grad():        for source_ids, target_ids, target_dists, relations in val_loader:            edge_samples = list(zip(source_ids.tolist(), target_ids.tolist(), target_dists.tolist(), relations))            d_loss = distance_loss(model, edge_samples)            sc_loss = semantic_clustering_loss(model, antonym_tensor)            rc_loss = relevance_coherence_loss(model, antonym_tensor)            rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])            commit_loss = relevance_commitment_loss(model)            r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])                        total = (                d_loss +                CONFIG['semantic_clustering_weight'] * sc_loss +                CONFIG['relevance_coherence_weight'] * rc_loss +                CONFIG['relevance_sparsity_weight'] * rs_loss +                CONFIG['relevance_commitment_weight'] * commit_loss +                CONFIG['reg_weight'] * r_loss            )            val_loss += total.item()        train_loss = total_loss / len(train_loader)    val_loss = val_loss / len(val_loader)    avg_soft, avg_hard = compute_avg_relevance(model)        history['train_loss'].append(train_loss)    history['val_loss'].append(val_loss)    history['avg_relevance_soft'].append(avg_soft)    history['avg_relevance_hard'].append(avg_hard)        print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")    print(f\"  Train Loss: {train_loss:.6f}\")    print(f\"  Val Loss: {val_loss:.6f}\")    print(f\"  Avg dims/word (soft): {avg_soft:.1f} (target: {CONFIG['target_dims_per_word']:.0f})\")    print(f\"  Avg dims/word (hard): {avg_hard:.1f} (>0.5 threshold)\")        # Save best    if val_loss < best_val_loss:        best_val_loss = val_loss        patience_counter = 0        print(f\"  ✓ New best model!\")        torch.save({            'epoch': epoch,            'model_state_dict': model.state_dict(),            'config': CONFIG,        }, f\"{CHECKPOINT_DIR}/best_model.pt\")    else:        patience_counter += 1        if patience_counter >= CONFIG['patience']:            print(f\"\\nEarly stopping at epoch {epoch}\")            break    print()elapsed = time.time() - start_timeprint(\"=\"*70)print(\"TRAINING COMPLETE\")print(\"=\"*70)print(f\"Time: {elapsed/3600:.1f} hours\")print(f\"Best val loss: {best_val_loss:.6f}\")print(f\"Final avg dims/word (soft): {history['avg_relevance_soft'][-1]:.1f}\")print(f\"Final avg dims/word (hard): {history['avg_relevance_hard'][-1]:.1f}\")print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xN_9rmAtBeB"
   },
   "source": [
    "## Step 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9naVai2atBeB"
   },
   "outputs": [],
   "source": [
    "# Save embeddings and relevance\n",
    "final_embeddings = model.embeddings.detach().cpu().numpy()\n",
    "final_relevance = torch.sigmoid(model.relevance_logits).detach().cpu().numpy()\n",
    "masked_embeddings = final_embeddings * final_relevance\n",
    "\n",
    "np.save(f\"{CHECKPOINT_DIR}/embeddings.npy\", final_embeddings)\n",
    "np.save(f\"{CHECKPOINT_DIR}/relevance.npy\", final_relevance)\n",
    "np.save(f\"{CHECKPOINT_DIR}/masked_embeddings.npy\", masked_embeddings)\n",
    "\n",
    "# Save vocabulary\n",
    "with open(f\"{CHECKPOINT_DIR}/vocabulary.json\", 'w') as f:\n",
    "    json.dump({'word_to_id': word_to_id, 'id_to_word': {str(k): v for k, v in id_to_word.items()}}, f)\n",
    "\n",
    "# Save history\n",
    "with open(f\"{CHECKPOINT_DIR}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Copy to Drive\n",
    "!cp -r {CHECKPOINT_DIR}/* {RESULTS_DIR}/\n",
    "\n",
    "print(f\"✓ Results saved to {RESULTS_DIR}\")\n",
    "print(f\"  - embeddings.npy ({final_embeddings.shape})\")\n",
    "print(f\"  - relevance.npy ({final_relevance.shape})\")\n",
    "print(f\"  - masked_embeddings.npy ({masked_embeddings.shape})\")\n",
    "print(f\"  - vocabulary.json ({len(vocabulary):,} words)\")\n",
    "print(f\"  - history.json\")\n",
    "print(f\"  - best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOiOHmITtBeB"
   },
   "source": [
    "## Step 7: Discover Semantic Axes\n",
    "\n",
    "Analyze which dimensions became semantic axes through natural clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSyI397DtBeB"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DISCOVERING SEMANTIC AXES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get final embeddings and relevance\n",
    "embeddings_np = model.embeddings.detach().cpu().numpy()\n",
    "relevance_np = torch.sigmoid(model.relevance_logits).detach().cpu().numpy()\n",
    "masked_embeddings_np = embeddings_np * relevance_np\n",
    "\n",
    "# Analyze antonym behavior on each dimension\n",
    "idx1 = antonym_tensor[:, 0].cpu().numpy()\n",
    "idx2 = antonym_tensor[:, 1].cpu().numpy()\n",
    "\n",
    "emb1 = masked_embeddings_np[idx1]\n",
    "emb2 = masked_embeddings_np[idx2]\n",
    "diffs = emb1 - emb2\n",
    "\n",
    "# Find dimensions with semantic structure\n",
    "semantic_axes = []\n",
    "for dim in range(CONFIG['num_anchors'], embeddings_np.shape[1]):\n",
    "    dim_diffs = diffs[:, dim]\n",
    "\n",
    "    # Key metrics\n",
    "    mean_abs_diff = np.mean(np.abs(dim_diffs))\n",
    "    sign_consistency = np.abs(np.mean(np.sign(dim_diffs)))\n",
    "\n",
    "    # How many words activate this dimension?\n",
    "    num_active = np.sum(relevance_np[:, dim] > 0.5)\n",
    "\n",
    "    # Semantic axis score\n",
    "    if mean_abs_diff > 0.01 and num_active > 10:\n",
    "        semantic_axes.append({\n",
    "            'dim': dim,\n",
    "            'consistency': sign_consistency,\n",
    "            'mean_diff': mean_abs_diff,\n",
    "            'num_active_words': num_active,\n",
    "            'score': mean_abs_diff * sign_consistency\n",
    "        })\n",
    "\n",
    "semantic_axes.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(semantic_axes)} candidate semantic axes:\\n\")\n",
    "print(f\"{'Dim':<6} {'Consistency':<12} {'Mean Diff':<12} {'Active Words':<15} {'Score':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for axis in semantic_axes[:20]:\n",
    "    print(f\"{axis['dim']:<6} {axis['consistency']:<12.4f} {axis['mean_diff']:<12.4f} \"\n",
    "          f\"{axis['num_active_words']:<15} {axis['score']:<10.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Natural emergence of {len([a for a in semantic_axes if a['consistency'] > 0.2])} strong axes (>20% consistency)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lACq7x8ctBeC"
   },
   "source": [
    "## Step 8: Detailed Semantic Axis Analysis\n",
    "\n",
    "Deep dive into the top semantic axes to see what concepts emerged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95QU2sJZtBeC"
   },
   "outputs": [],
   "source": [
    "# Analyze top 5 semantic axes in detail\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 5 SEMANTIC AXES - DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for rank, axis in enumerate(semantic_axes[:5], 1):\n",
    "    dim_idx = axis['dim']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Semantic Axis {dim_idx} (Rank #{rank})\")\n",
    "    print(f\"  Consistency: {axis['consistency']:.1%}\")\n",
    "    print(f\"  Active words: {axis['num_active_words']}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Get dimension values (with relevance gating)\n",
    "    dim_values = masked_embeddings_np[:, dim_idx]\n",
    "    dim_relevance = relevance_np[:, dim_idx]\n",
    "\n",
    "    # Show words that activate this dimension\n",
    "    active_words = np.where(dim_relevance > 0.5)[0]\n",
    "    active_values = dim_values[active_words]\n",
    "\n",
    "    sorted_active = active_words[np.argsort(active_values)]\n",
    "\n",
    "    print(\"\\n  POSITIVE POLE (words with high relevance):\")\n",
    "    for idx in sorted_active[-10:][::-1]:\n",
    "        word = id_to_word[idx]\n",
    "        word_display = word.split('_wn.')[0] if '_wn.' in word else word\n",
    "        rel = dim_relevance[idx]\n",
    "        val = dim_values[idx]\n",
    "        print(f\"    {word_display:25s} value={val:+.3f}  relevance={rel:.3f}\")\n",
    "\n",
    "    print(\"\\n  NEGATIVE POLE:\")\n",
    "    for idx in sorted_active[:10]:\n",
    "        word = id_to_word[idx]\n",
    "        word_display = word.split('_wn.')[0] if '_wn.' in word else word\n",
    "        rel = dim_relevance[idx]\n",
    "        val = dim_values[idx]\n",
    "        print(f\"    {word_display:25s} value={val:+.3f}  relevance={rel:.3f}\")\n",
    "\n",
    "    # Show antonym pairs on this axis\n",
    "    print(\"\\n  ANTONYM PAIRS (sample where both have high relevance):\")\n",
    "    shown = 0\n",
    "    for i in range(len(idx1)):\n",
    "        w1_rel = relevance_np[idx1[i], dim_idx]\n",
    "        w2_rel = relevance_np[idx2[i], dim_idx]\n",
    "\n",
    "        if w1_rel > 0.5 and w2_rel > 0.5:  # Both relevant\n",
    "            word1 = id_to_word[idx1[i]]\n",
    "            word2 = id_to_word[idx2[i]]\n",
    "            val1 = masked_embeddings_np[idx1[i], dim_idx]\n",
    "            val2 = masked_embeddings_np[idx2[i], dim_idx]\n",
    "\n",
    "            w1_display = word1.split('_wn.')[0] if '_wn.' in word1 else word1\n",
    "            w2_display = word2.split('_wn.')[0] if '_wn.' in word2 else word2\n",
    "\n",
    "            print(f\"    {w1_display:15s} ({val1:+.3f}) ↔ {w2_display:15s} ({val2:+.3f})\")\n",
    "            shown += 1\n",
    "            if shown >= 10:\n",
    "                break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}