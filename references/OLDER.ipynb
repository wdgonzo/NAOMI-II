{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4YCTKVspd0H"
   },
   "source": [
    "# NAOMI-II: Semantic-Driven Transparent Dimension Training\n",
    "\n",
    "**Revolutionary approach: Dimensions emerge through semantic clustering!**\n",
    "\n",
    "Uses your existing pre-parsed Wikipedia + WordNet data:\n",
    "- **197K words** (WordNet + Wikipedia vocabulary)\n",
    "- **1.79M triples** (1.56M parse + 230K WordNet)\n",
    "- **971 antonym pairs** for semantic axis discovery\n",
    "- **100K Wikipedia sentences** (already parsed)\n",
    "\n",
    "## Key Innovation\n",
    "\n",
    "**Old approach (WRONG):**\n",
    "- Force 15 \"polarity dimensions\"\n",
    "- Train for opposition as goal\n",
    "- Sparsity as penalty\n",
    "\n",
    "**New approach (RIGHT):**\n",
    "- ALL dimensions = semantic axes (morality, temperature, size, etc.)\n",
    "- Words positioned by meaning\n",
    "- Opposition emerges from opposite meanings\n",
    "- Sparsity emerges from semantic irrelevance\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Semantic-driven embeddings:**\n",
    "- `embeddings`: Position on each semantic axis\n",
    "- `relevance`: Which axes matter for this word's meaning\n",
    "- Final embedding = embeddings × relevance (automatic semantic sparsity!)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "\"good\" on morality axis: value=+0.8, relevance=1.0 → +0.8 ✓\n",
    "\"good\" on temperature: value=0.1, relevance=0.0 → 0.0 ✓ (semantically correct!)\n",
    "```\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- 10-30 semantic axes emerge naturally\n",
    "- Each axis = one global concept (morality, size, temperature, politics, quality, etc.)\n",
    "- Words activate 5-20 axes on average (semantic sparsity)\n",
    "- Antonyms naturally oppose on shared relevant axes\n",
    "- Clear, interpretable dimensional structure\n",
    "\n",
    "**Runtime:** ~10-12 hours on A100 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Upload to Google Drive at `/NAOMI-II-data/wikipedia_100k_graph/`:\n",
    "- `vocabulary.json` (197K words)\n",
    "- `triples.pkl` (1.79M triples)\n",
    "- `training_examples.pkl` (1.66M examples)\n",
    "- `graph_stats.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgV8wch5pd0L"
   },
   "source": [
    "## Step 1: Setup and Load Pre-Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYYKNxwtpd0M",
    "outputId": "80f04ff3-5060-4de6-81bd-c30e8d576b2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "✓ GPU: NVIDIA A100-SXM4-80GB (85.2 GB)\n",
      "✓ Device: cuda\n",
      "✓ Data path: /content/drive/MyDrive/NAOMI-II-data/wikipedia_100k_graph\n",
      "✓ Results path: /content/drive/MyDrive/NAOMI-II-results/semantic_dims\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy nltk tqdm scikit-learn matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"/content/drive/MyDrive/NAOMI-II-data/wikipedia_100k_graph\"\n",
    "RESULTS_DIR = \"/content/drive/MyDrive/NAOMI-II-results/semantic_dims\"\n",
    "CHECKPOINT_DIR = \"/content/checkpoints\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"✓ GPU: {gpu_name} ({vram_gb:.1f} GB)\")\n",
    "    if 'A100' not in gpu_name:\n",
    "        print(f\"⚠️  WARNING: Not A100! Training will be slower.\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU detected!\")\n",
    "\n",
    "print(f\"✓ Device: {device}\")\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Results path: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEjiKM96pd0N",
    "outputId": "48af8597-b65d-41bf-ab64-99ae52946bed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "LOADING PRE-PARSED WIKIPEDIA + WORDNET DATA\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading vocabulary...\n",
      "  ✓ Vocabulary: 197,095 words\n",
      "\n",
      "[2/4] Loading triples...\n",
      "  ✓ Triples: 1,785,340\n",
      "\n",
      "[3/4] Loading training examples...\n",
      "  ✓ Training examples: 1,662,260\n",
      "\n",
      "[4/4] Extracting antonym pairs...\n",
      "  ✓ Antonym pairs: 971\n",
      "\n",
      "======================================================================\n",
      "DATA LOADED SUCCESSFULLY\n",
      "======================================================================\n",
      "  Vocabulary: 197,095 sense-tagged words\n",
      "  Triples: 1,785,340\n",
      "  Training examples: 1,662,260\n",
      "  Source sentences: 100,000\n",
      "  Antonym pairs: 971 ← KEY FOR SEMANTIC AXES!\n",
      "======================================================================\n",
      "\n",
      "Sample antonym pairs:\n",
      "  comparably ↔ incomparably\n",
      "  leeward ↔ windward\n",
      "  noblewoman ↔ nobleman\n",
      "  cash ↔ credit\n",
      "  diapsid ↔ anapsid\n",
      "  hardware ↔ software\n",
      "  ascent ↔ descent\n",
      "  complexity ↔ simplicity\n",
      "  intelligence ↔ stupidity\n",
      "  king ↔ queen\n"
     ]
    }
   ],
   "source": [
    "# Load pre-parsed data from Google Drive\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING PRE-PARSED WIKIPEDIA + WORDNET DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load vocabulary\n",
    "print(\"\\n[1/4] Loading vocabulary...\")\n",
    "with open(f\"{DATA_PATH}/vocabulary.json\", 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "    word_to_id = vocab_data['word_to_id']\n",
    "    vocabulary = list(word_to_id.keys())\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "print(f\"  ✓ Vocabulary: {len(vocabulary):,} words\")\n",
    "\n",
    "# Load triples\n",
    "print(\"\\n[2/4] Loading triples...\")\n",
    "with open(f\"{DATA_PATH}/triples.pkl\", 'rb') as f:\n",
    "    triples = pickle.load(f)\n",
    "\n",
    "print(f\"  ✓ Triples: {len(triples):,}\")\n",
    "\n",
    "# Load training examples\n",
    "print(\"\\n[3/4] Loading training examples...\")\n",
    "with open(f\"{DATA_PATH}/training_examples.pkl\", 'rb') as f:\n",
    "    training_examples = pickle.load(f)\n",
    "\n",
    "print(f\"  ✓ Training examples: {len(training_examples):,}\")\n",
    "\n",
    "# Extract antonym pairs\n",
    "print(\"\\n[4/4] Extracting antonym pairs...\")\n",
    "antonym_pairs = []\n",
    "for triple in triples:\n",
    "    source_word, relation, target_word = triple\n",
    "    if 'antonym' in relation.lower():\n",
    "        antonym_pairs.append((source_word, target_word))\n",
    "\n",
    "print(f\"  ✓ Antonym pairs: {len(antonym_pairs):,}\")\n",
    "\n",
    "# Load stats\n",
    "with open(f\"{DATA_PATH}/graph_stats.json\", 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Vocabulary: {stats['vocabulary_size']:,} sense-tagged words\")\n",
    "print(f\"  Triples: {stats['num_triples']:,}\")\n",
    "print(f\"  Training examples: {stats['num_training_examples']:,}\")\n",
    "print(f\"  Source sentences: {stats['num_sentences']:,}\")\n",
    "print(f\"  Antonym pairs: {len(antonym_pairs):,} ← KEY FOR SEMANTIC AXES!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample antonyms\n",
    "print(\"\\nSample antonym pairs:\")\n",
    "for word1, word2 in antonym_pairs[:10]:\n",
    "    w1_display = word1.split('_wn.')[0] if '_wn.' in word1 else word1\n",
    "    w2_display = word2.split('_wn.')[0] if '_wn.' in word2 else word2\n",
    "    print(f\"  {w1_display} ↔ {w2_display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-f3UM9Xpd0N"
   },
   "source": [
    "## Step 2: Initialize Semantic-Driven Model\n",
    "\n",
    "**Key innovation:** Each word learns which semantic axes are relevant to its meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiXVRyE6pd0O",
    "outputId": "ecc09340-1b4a-4dfa-d633-1a6c891fcc70"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration:\n",
      "======================================================================\n",
      "  vocab_size: 197095\n",
      "  embedding_dim: 256\n",
      "  num_anchors: 51\n",
      "  epochs: 150\n",
      "  batch_size: 262144\n",
      "  lr: 0.01\n",
      "  semantic_clustering_weight: 2.0\n",
      "  relevance_coherence_weight: 1.0\n",
      "  target_dims_per_word: 10.0\n",
      "  relevance_sparsity_weight: 0.05\n",
      "  reg_weight: 0.05\n",
      "  parse_weight: 0.3\n",
      "  wordnet_weight: 0.7\n",
      "  patience: 30\n",
      "  mixed_precision: True\n",
      "======================================================================\n",
      "\n",
      "✓ Semantic-driven model initialized\n",
      "  Vocabulary: 197,095 words\n",
      "  Embedding dim: 256 (51 anchor + 205 learned)\n",
      "  Parameters: 100,912,640\n",
      "\n",
      "  KEY FEATURES:\n",
      "  - Each word learns which semantic axes are relevant\n",
      "  - Target: 10 dimensions per word (semantic sweet spot)\n",
      "  - Initialized sparse (bias toward -3.0) to break symmetry\n",
      "  - Stronger sparsity weight (0.05) to push toward target\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'vocab_size': len(vocabulary),\n",
    "    'embedding_dim': 256,\n",
    "    'num_anchors': 51,\n",
    "    'epochs': 150,\n",
    "    'batch_size': 262144,\n",
    "    'lr': 0.01,\n",
    "\n",
    "    # Semantic-driven weights\n",
    "    'semantic_clustering_weight': 2.0,\n",
    "    'relevance_coherence_weight': 1.0,\n",
    "    'target_dims_per_word': 10.0,      # Target: 10 dimensions per word on average\n",
    "    'relevance_sparsity_weight': 0.05,  # Increased from 0.01 - stronger push toward target\n",
    "    'relevance_commitment_weight': 0.5,  # Force binary commitment\n",
    "    'reg_weight': 0.05,\n",
    "    'parse_weight': 0.3,\n",
    "    'wordnet_weight': 0.7,\n",
    "    'patience': 30,\n",
    "    'mixed_precision': True,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\"=\"*70)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model\n",
    "class SemanticTransparentEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeddings with semantic-driven sparsity.\n",
    "\n",
    "    Key components:\n",
    "    - embeddings: Position of word on each semantic axis\n",
    "    - relevance_logits: Which axes are relevant to this word's meaning\n",
    "    - Final embedding = embeddings * sigmoid(relevance)\n",
    "\n",
    "    Example:\n",
    "      'good' on morality axis: embedding=+0.8, relevance=1.0 → +0.8\n",
    "      'good' on temperature axis: embedding=0.1, relevance=0.0 → 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, num_anchors):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        # Core embeddings (position on each semantic axis)\n",
    "        self.embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.01)\n",
    "\n",
    "        # Dimension relevance (which axes matter for this word?)\n",
    "        # Initialize with negative bias for sparsity\n",
    "        # sigmoid(-3.0) ≈ 0.047 → starts with ~10 dims/word instead of ~100\n",
    "        self.relevance_logits = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.5 - 3.0)\n",
    "\n",
    "        # Initialize anchors\n",
    "        with torch.no_grad():\n",
    "            self.embeddings[:, :num_anchors] = 0.0\n",
    "            self.relevance_logits[:, :num_anchors] = -10.0  # Anchors always off\n",
    "\n",
    "    def get_masked_embeddings(self, word_ids):\n",
    "        \"\"\"Get embeddings gated by semantic relevance.\"\"\"\n",
    "        emb = self.embeddings[word_ids]\n",
    "        relevance = torch.sigmoid(self.relevance_logits[word_ids])\n",
    "        return emb * relevance\n",
    "\n",
    "    def get_learned_dims_mask(self):\n",
    "        mask = torch.zeros(self.embedding_dim, dtype=torch.bool)\n",
    "        mask[self.num_anchors:] = True\n",
    "        return mask\n",
    "\n",
    "model = SemanticTransparentEmbedding(\n",
    "    CONFIG['vocab_size'],\n",
    "    CONFIG['embedding_dim'],\n",
    "    CONFIG['num_anchors']\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n✓ Semantic-driven model initialized\")\n",
    "print(f\"  Vocabulary: {CONFIG['vocab_size']:,} words\")\n",
    "print(f\"  Embedding dim: {CONFIG['embedding_dim']} ({CONFIG['num_anchors']} anchor + {CONFIG['embedding_dim']-CONFIG['num_anchors']} learned)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\n  KEY FEATURES:\")\n",
    "print(f\"  - Each word learns which semantic axes are relevant\")\n",
    "print(f\"  - Target: {CONFIG['target_dims_per_word']:.0f} dimensions per word (semantic sweet spot)\")\n",
    "print(f\"  - Initialized sparse (bias toward -3.0) to break symmetry\")\n",
    "print(f\"  - Stronger sparsity weight (0.05) to push toward target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbr7zBWKpd0O"
   },
   "source": [
    "## Step 3: Semantic-Driven Loss Functions\n",
    "\n",
    "No forced polarity structure - dimensions emerge through semantic clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26iLUb7Epd0O",
    "outputId": "25205ad7-d07b-480b-f606-c446a101cf16"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching antonym pairs to vocabulary...\n",
      "✓ Matched 648 / 971 antonym pairs\n",
      "\n",
      "✓ Semantic-driven loss functions initialized\n",
      "  - Semantic clustering: Pairs with similar meanings use same dimensions\n",
      "  - Relevance coherence: Antonyms activate same dimensions\n",
      "  - Relevance sparsity: TARGET-BASED (aim for 10 dims/word, not minimize!)\n",
      "  - Uses SOFT counting for differentiable gradients\n",
      "  - Natural emergence of semantic axes!\n"
     ]
    }
   ],
   "source": [
    "# Match antonym pairs to IDs (with fuzzy matching)\n",
    "print(\"Matching antonym pairs to vocabulary...\")\n",
    "antonym_indices = []\n",
    "for word1, word2 in antonym_pairs:\n",
    "    idx1 = word_to_id.get(word1)\n",
    "    idx2 = word_to_id.get(word2)\n",
    "\n",
    "    # Fuzzy match if needed\n",
    "    if idx1 is None:\n",
    "        for w in word_to_id:\n",
    "            if w.startswith(word1.split('_wn')[0] + \"_wn.\"):\n",
    "                idx1 = word_to_id[w]\n",
    "                break\n",
    "    if idx2 is None:\n",
    "        for w in word_to_id:\n",
    "            if w.startswith(word2.split('_wn')[0] + \"_wn.\"):\n",
    "                idx2 = word_to_id[w]\n",
    "                break\n",
    "\n",
    "    if idx1 is not None and idx2 is not None:\n",
    "        antonym_indices.append((idx1, idx2))\n",
    "\n",
    "antonym_tensor = torch.tensor(antonym_indices, dtype=torch.long, device=device)\n",
    "print(f\"✓ Matched {len(antonym_indices):,} / {len(antonym_pairs):,} antonym pairs\\n\")\n",
    "\n",
    "# Loss functions\n",
    "def distance_loss(model, edge_samples):\n",
    "    \"\"\"Semantic distance loss with parse/WordNet weighting.\"\"\"\n",
    "    if len(edge_samples) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    parse_samples = []\n",
    "    wordnet_samples = []\n",
    "\n",
    "    for source_id, target_id, target_dist, relation in edge_samples:\n",
    "        if relation.startswith('RelationType.'):\n",
    "            parse_samples.append((source_id, target_id, target_dist))\n",
    "        else:\n",
    "            wordnet_samples.append((source_id, target_id, target_dist))\n",
    "\n",
    "    total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    if parse_samples:\n",
    "        source_ids = torch.tensor([s for s, t, d in parse_samples], device=device)\n",
    "        target_ids = torch.tensor([t for s, t, d in parse_samples], device=device)\n",
    "        target_dists = torch.tensor([d for s, t, d in parse_samples], device=device, dtype=torch.float32)\n",
    "        source_emb = model.get_masked_embeddings(source_ids)\n",
    "        target_emb = model.get_masked_embeddings(target_ids)\n",
    "        actual_dists = torch.norm(source_emb - target_emb, dim=1)\n",
    "        parse_loss = F.mse_loss(actual_dists, target_dists)\n",
    "        total_loss += CONFIG['parse_weight'] * parse_loss\n",
    "\n",
    "    if wordnet_samples:\n",
    "        source_ids = torch.tensor([s for s, t, d in wordnet_samples], device=device)\n",
    "        target_ids = torch.tensor([t for s, t, d in wordnet_samples], device=device)\n",
    "        target_dists = torch.tensor([d for s, t, d in wordnet_samples], device=device, dtype=torch.float32)\n",
    "        source_emb = model.get_masked_embeddings(source_ids)\n",
    "        target_emb = model.get_masked_embeddings(target_ids)\n",
    "        actual_dists = torch.norm(source_emb - target_emb, dim=1)\n",
    "        wordnet_loss = F.mse_loss(actual_dists, target_dists)\n",
    "        total_loss += CONFIG['wordnet_weight'] * wordnet_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def semantic_clustering_loss(model, antonym_tensor):\n",
    "    \"\"\"\n",
    "    Encourage semantically-related antonym pairs to use same dimensions.\n",
    "\n",
    "    Mechanism:\n",
    "    1. Compute pair centroids (semantic similarity)\n",
    "    2. Compute dimensional signatures (which dims they use)\n",
    "    3. Force signature similarity to match semantic similarity\n",
    "\n",
    "    Result: (good/bad) and (right/wrong) both use \"morality\" dimension!\n",
    "    \"\"\"\n",
    "    if len(antonym_tensor) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Get masked embeddings (respects relevance)\n",
    "    emb1 = model.get_masked_embeddings(antonym_tensor[:, 0])\n",
    "    emb2 = model.get_masked_embeddings(antonym_tensor[:, 1])\n",
    "\n",
    "    # Semantic signatures (what concepts are these pairs about?)\n",
    "    pair_centroids = (emb1 + emb2) / 2\n",
    "    semantic_sim = torch.mm(F.normalize(pair_centroids, dim=1),\n",
    "                           F.normalize(pair_centroids, dim=1).T)\n",
    "\n",
    "    # Dimensional signatures (which dimensions do they use?)\n",
    "    diff_vectors = emb1 - emb2\n",
    "    dimensional_sim = torch.mm(F.normalize(diff_vectors, dim=1),\n",
    "                              F.normalize(diff_vectors, dim=1).T)\n",
    "\n",
    "    # Force dimensional usage to match semantic similarity\n",
    "    # Similar pairs → use same dimensions\n",
    "    # Different pairs → use different dimensions\n",
    "    return F.mse_loss(dimensional_sim, semantic_sim)\n",
    "\n",
    "def relevance_coherence_loss(model, antonym_tensor):\n",
    "    \"\"\"\n",
    "    Antonym pairs should activate the same dimensions.\n",
    "\n",
    "    If 'good' activates morality dimension, 'bad' should too\n",
    "    (even though they have opposite values).\n",
    "    \"\"\"\n",
    "    if len(antonym_tensor) == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    rel1 = torch.sigmoid(model.relevance_logits[antonym_tensor[:, 0]])\n",
    "    rel2 = torch.sigmoid(model.relevance_logits[antonym_tensor[:, 1]])\n",
    "\n",
    "    # Cosine similarity of relevance patterns\n",
    "    relevance_similarity = F.cosine_similarity(rel1, rel2, dim=1)\n",
    "\n",
    "    # Maximize similarity (both activate same axes)\n",
    "    return -torch.mean(relevance_similarity)\n",
    "\n",
    "def relevance_sparsity_loss(model, target_dims):\n",
    "    \"\"\"\n",
    "    Target-based sparsity: aim for specific number of dimensions per word.\n",
    "\n",
    "    Don't just minimize activation - target the semantic sweet spot!\n",
    "    Target: 10 dimensions per word on average.\n",
    "\n",
    "    Uses SOFT counting (sum of sigmoid values) for differentiability.\n",
    "    \"\"\"\n",
    "    relevance = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])\n",
    "\n",
    "    # Average number of active dimensions per word (soft count)\n",
    "    # Sum sigmoid values across dimensions for each word, then average\n",
    "    avg_active_dims = torch.mean(torch.sum(relevance, dim=1))\n",
    "\n",
    "    # Penalize deviation from target\n",
    "    # If avg = 10 and target = 10, loss = 0 (perfect!)\n",
    "    # If avg = 102 and target = 10, loss = high (too dense!)\n",
    "    # If avg = 0 and target = 10, loss = high (too sparse!)\n",
    "    target_tensor = torch.tensor(target_dims, device=avg_active_dims.device, dtype=avg_active_dims.dtype)\n",
    "    return F.mse_loss(avg_active_dims, target_tensor)\n",
    "\n",
    "\n",
    "def relevance_commitment_loss(model):\n",
    "    \"\"\"\n",
    "    Force dimensions to commit: active (1.0) or inactive (0.0).\n",
    "    \n",
    "    Uses entropy penalty - sigmoid at 0.5 has HIGH entropy (uncertain).\n",
    "    We want LOW entropy (committed to 0 or 1).\n",
    "    \"\"\"\n",
    "    relevance = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])\n",
    "    \n",
    "    # Entropy of Bernoulli distribution\n",
    "    eps = 1e-8\n",
    "    entropy = -(relevance * torch.log(relevance + eps) + \n",
    "                (1 - relevance) * torch.log(1 - relevance + eps))\n",
    "    \n",
    "    return torch.mean(entropy)\n",
    "\n",
    "def regularization_loss(embeddings, num_anchors):\n",
    "    \"\"\"L2 regularization on learned dimensions.\"\"\"\n",
    "    return torch.mean(embeddings[:, num_anchors:] ** 2)\n",
    "\n",
    "print(\"✓ Semantic-driven loss functions initialized\")\n",
    "print(\"  - Semantic clustering: Pairs with similar meanings use same dimensions\")\n",
    "print(\"  - Relevance coherence: Antonyms activate same dimensions\")\n",
    "print(\"  - Relevance sparsity: TARGET-BASED (aim for 10 dims/word, not minimize!)\")\n",
    "print(\"  - Uses SOFT counting for differentiable gradients\")\n",
    "  print(\"  - Relevance commitment: Binary decisions (forces 0 or 1)\")\n",
    "print(\"  - Natural emergence of semantic axes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q00VBXMpd0P"
   },
   "source": [
    "## Step 4: Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JcX7-2epd0Q",
    "outputId": "95962b27-1aca-4f35-d580-e9d93f56b086"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Train examples: 1,496,034\n",
      "✓ Val examples: 166,226\n",
      "✓ Train batches: 6\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeGraphDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_id, relation, target_id = self.examples[idx]\n",
    "        target_distance = 0.5\n",
    "        if 'synonym' in relation.lower():\n",
    "            target_distance = 0.1\n",
    "        elif 'antonym' in relation.lower():\n",
    "            target_distance = 0.9\n",
    "        return source_id, target_id, target_distance, relation\n",
    "\n",
    "# Split dataset\n",
    "random.seed(42)\n",
    "random.shuffle(training_examples)\n",
    "\n",
    "split_idx = int(0.9 * len(training_examples))\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "train_dataset = KnowledgeGraphDataset(train_examples)\n",
    "val_dataset = KnowledgeGraphDataset(val_examples)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=12, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "print(f\"✓ Train examples: {len(train_examples):,}\")\n",
    "print(f\"✓ Val examples: {len(val_examples):,}\")\n",
    "print(f\"✓ Train batches: {len(train_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UADRV3hgpd0Q"
   },
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Dimensions emerge through semantic clustering - no forced structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJMZV4w0pd0Q",
    "outputId": "7cf9acf7-f80b-4e20-84cc-c4b2d7613540"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "STARTING SEMANTIC-DRIVEN TRAINING\n",
      "======================================================================\n",
      "Key difference: Dimensions emerge through semantic clustering\n",
      "No forced polarity structure - natural semantic axes!\n",
      "Target: 10 dimensions per word (soft count)\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 6/6 [00:05<00:00,  1.07it/s, loss=-0.6554, sc=0.0096]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 1/150\n",
      "  Train Loss: -0.608882\n",
      "  Val Loss: -0.670916\n",
      "  Avg dims/word (soft): 10.4 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 6/6 [00:04<00:00,  1.32it/s, loss=-0.7454, sc=0.0087]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 2/150\n",
      "  Train Loss: -0.710872\n",
      "  Val Loss: -0.752981\n",
      "  Avg dims/word (soft): 10.1 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.8116, sc=0.0082]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 3/150\n",
      "  Train Loss: -0.785961\n",
      "  Val Loss: -0.815974\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 6/6 [00:04<00:00,  1.33it/s, loss=-0.8638, sc=0.0078]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 4/150\n",
      "  Train Loss: -0.843739\n",
      "  Val Loss: -0.864827\n",
      "  Avg dims/word (soft): 9.9 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 6/6 [00:04<00:00,  1.33it/s, loss=-0.9024, sc=0.0076]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 5/150\n",
      "  Train Loss: -0.887453\n",
      "  Val Loss: -0.900860\n",
      "  Avg dims/word (soft): 9.9 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9274, sc=0.0075]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 6/150\n",
      "  Train Loss: -0.917956\n",
      "  Val Loss: -0.924524\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 6/6 [00:04<00:00,  1.32it/s, loss=-0.9416, sc=0.0073]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 7/150\n",
      "  Train Loss: -0.936592\n",
      "  Val Loss: -0.938152\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 6/6 [00:04<00:00,  1.34it/s, loss=-0.9499, sc=0.0073]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 8/150\n",
      "  Train Loss: -0.947045\n",
      "  Val Loss: -0.946048\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 6/6 [00:04<00:00,  1.34it/s, loss=-0.9559, sc=0.0072]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 9/150\n",
      "  Train Loss: -0.953781\n",
      "  Val Loss: -0.951661\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 6/6 [00:04<00:00,  1.32it/s, loss=-0.9611, sc=0.0072]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 10/150\n",
      "  Train Loss: -0.959207\n",
      "  Val Loss: -0.956011\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 6/6 [00:04<00:00,  1.35it/s, loss=-0.9649, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 11/150\n",
      "  Train Loss: -0.963546\n",
      "  Val Loss: -0.959153\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 6/6 [00:04<00:00,  1.34it/s, loss=-0.9675, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 12/150\n",
      "  Train Loss: -0.966737\n",
      "  Val Loss: -0.961354\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9698, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 13/150\n",
      "  Train Loss: -0.969095\n",
      "  Val Loss: -0.963020\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9715, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 14/150\n",
      "  Train Loss: -0.970918\n",
      "  Val Loss: -0.964389\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 6/6 [00:04<00:00,  1.30it/s, loss=-0.9722, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 15/150\n",
      "  Train Loss: -0.972373\n",
      "  Val Loss: -0.965517\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 6/6 [00:04<00:00,  1.32it/s, loss=-0.9742, sc=0.0071]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 16/150\n",
      "  Train Loss: -0.973633\n",
      "  Val Loss: -0.966435\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 6/6 [00:04<00:00,  1.30it/s, loss=-0.9752, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 17/150\n",
      "  Train Loss: -0.974636\n",
      "  Val Loss: -0.967180\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9757, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 18/150\n",
      "  Train Loss: -0.975452\n",
      "  Val Loss: -0.967798\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9766, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 19/150\n",
      "  Train Loss: -0.976168\n",
      "  Val Loss: -0.968317\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 6/6 [00:04<00:00,  1.30it/s, loss=-0.9766, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 20/150\n",
      "  Train Loss: -0.976747\n",
      "  Val Loss: -0.968748\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 21: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.9776, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 21/150\n",
      "  Train Loss: -0.977285\n",
      "  Val Loss: -0.969114\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 22: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.9775, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 22/150\n",
      "  Train Loss: -0.977705\n",
      "  Val Loss: -0.969428\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 23: 100%|██████████| 6/6 [00:05<00:00,  1.08it/s, loss=-0.9784, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 23/150\n",
      "  Train Loss: -0.978120\n",
      "  Val Loss: -0.969700\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 24: 100%|██████████| 6/6 [00:04<00:00,  1.30it/s, loss=-0.9783, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 24/150\n",
      "  Train Loss: -0.978442\n",
      "  Val Loss: -0.969940\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 25: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9789, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 25/150\n",
      "  Train Loss: -0.978756\n",
      "  Val Loss: -0.970147\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 26: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9794, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 26/150\n",
      "  Train Loss: -0.979036\n",
      "  Val Loss: -0.970331\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 27: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9792, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 27/150\n",
      "  Train Loss: -0.979254\n",
      "  Val Loss: -0.970495\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 28: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9798, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 28/150\n",
      "  Train Loss: -0.979486\n",
      "  Val Loss: -0.970641\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 29: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9797, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 29/150\n",
      "  Train Loss: -0.979669\n",
      "  Val Loss: -0.970774\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 30: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9800, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 30/150\n",
      "  Train Loss: -0.979854\n",
      "  Val Loss: -0.970894\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 31: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s, loss=-0.9799, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 31/150\n",
      "  Train Loss: -0.980008\n",
      "  Val Loss: -0.971004\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 32: 100%|██████████| 6/6 [00:05<00:00,  1.16it/s, loss=-0.9798, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 32/150\n",
      "  Train Loss: -0.980146\n",
      "  Val Loss: -0.971102\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 33: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9806, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 33/150\n",
      "  Train Loss: -0.980319\n",
      "  Val Loss: -0.971194\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 34: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9806, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 34/150\n",
      "  Train Loss: -0.980450\n",
      "  Val Loss: -0.971280\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 35: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9804, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 35/150\n",
      "  Train Loss: -0.980563\n",
      "  Val Loss: -0.971362\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 36: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9806, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 36/150\n",
      "  Train Loss: -0.980689\n",
      "  Val Loss: -0.971445\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 37: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s, loss=-0.9806, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 37/150\n",
      "  Train Loss: -0.980797\n",
      "  Val Loss: -0.971519\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 38: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9807, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 38/150\n",
      "  Train Loss: -0.980912\n",
      "  Val Loss: -0.971594\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 39: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.9809, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 39/150\n",
      "  Train Loss: -0.981021\n",
      "  Val Loss: -0.971665\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 40: 100%|██████████| 6/6 [00:05<00:00,  1.11it/s, loss=-0.9813, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 40/150\n",
      "  Train Loss: -0.981140\n",
      "  Val Loss: -0.971734\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 41: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.9815, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 41/150\n",
      "  Train Loss: -0.981247\n",
      "  Val Loss: -0.971797\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 42: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9813, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 42/150\n",
      "  Train Loss: -0.981327\n",
      "  Val Loss: -0.971857\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 43: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s, loss=-0.9812, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 43/150\n",
      "  Train Loss: -0.981409\n",
      "  Val Loss: -0.971910\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 44: 100%|██████████| 6/6 [00:04<00:00,  1.24it/s, loss=-0.9819, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 44/150\n",
      "  Train Loss: -0.981520\n",
      "  Val Loss: -0.971961\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 45: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9817, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 45/150\n",
      "  Train Loss: -0.981586\n",
      "  Val Loss: -0.972012\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 46: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9817, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 46/150\n",
      "  Train Loss: -0.981662\n",
      "  Val Loss: -0.972059\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 47: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9817, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 47/150\n",
      "  Train Loss: -0.981734\n",
      "  Val Loss: -0.972102\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 48: 100%|██████████| 6/6 [00:04<00:00,  1.25it/s, loss=-0.9818, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 48/150\n",
      "  Train Loss: -0.981805\n",
      "  Val Loss: -0.972146\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 49: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9821, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 49/150\n",
      "  Train Loss: -0.981887\n",
      "  Val Loss: -0.972186\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 50: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9820, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 50/150\n",
      "  Train Loss: -0.981946\n",
      "  Val Loss: -0.972226\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 51: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s, loss=-0.9820, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 51/150\n",
      "  Train Loss: -0.982009\n",
      "  Val Loss: -0.972265\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 52: 100%|██████████| 6/6 [00:04<00:00,  1.25it/s, loss=-0.9817, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 52/150\n",
      "  Train Loss: -0.982053\n",
      "  Val Loss: -0.972300\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 53: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9822, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 53/150\n",
      "  Train Loss: -0.982138\n",
      "  Val Loss: -0.972333\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 54: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s, loss=-0.9825, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 54/150\n",
      "  Train Loss: -0.982205\n",
      "  Val Loss: -0.972366\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 55: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9823, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 55/150\n",
      "  Train Loss: -0.982250\n",
      "  Val Loss: -0.972400\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 56: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9822, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 56/150\n",
      "  Train Loss: -0.982303\n",
      "  Val Loss: -0.972433\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 57: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9823, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 57/150\n",
      "  Train Loss: -0.982357\n",
      "  Val Loss: -0.972461\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 58: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9823, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 58/150\n",
      "  Train Loss: -0.982411\n",
      "  Val Loss: -0.972487\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 59: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9825, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 59/150\n",
      "  Train Loss: -0.982467\n",
      "  Val Loss: -0.972513\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 60: 100%|██████████| 6/6 [00:04<00:00,  1.24it/s, loss=-0.9826, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 60/150\n",
      "  Train Loss: -0.982522\n",
      "  Val Loss: -0.972545\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 61: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9825, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 61/150\n",
      "  Train Loss: -0.982566\n",
      "  Val Loss: -0.972574\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 62: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9825, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 62/150\n",
      "  Train Loss: -0.982612\n",
      "  Val Loss: -0.972603\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 63: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9826, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 63/150\n",
      "  Train Loss: -0.982662\n",
      "  Val Loss: -0.972625\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 64: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9828, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 64/150\n",
      "  Train Loss: -0.982714\n",
      "  Val Loss: -0.972652\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 65: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9828, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 65/150\n",
      "  Train Loss: -0.982757\n",
      "  Val Loss: -0.972675\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 66: 100%|██████████| 6/6 [00:04<00:00,  1.24it/s, loss=-0.9829, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 66/150\n",
      "  Train Loss: -0.982803\n",
      "  Val Loss: -0.972699\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 67: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9829, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 67/150\n",
      "  Train Loss: -0.982841\n",
      "  Val Loss: -0.972724\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 68: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9830, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 68/150\n",
      "  Train Loss: -0.982888\n",
      "  Val Loss: -0.972744\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 69: 100%|██████████| 6/6 [00:04<00:00,  1.28it/s, loss=-0.9832, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 69/150\n",
      "  Train Loss: -0.982933\n",
      "  Val Loss: -0.972768\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 70: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9830, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 70/150\n",
      "  Train Loss: -0.982966\n",
      "  Val Loss: -0.972789\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 71: 100%|██████████| 6/6 [00:04<00:00,  1.25it/s, loss=-0.9831, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 71/150\n",
      "  Train Loss: -0.983006\n",
      "  Val Loss: -0.972811\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 72: 100%|██████████| 6/6 [00:04<00:00,  1.30it/s, loss=-0.9830, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 72/150\n",
      "  Train Loss: -0.983035\n",
      "  Val Loss: -0.972830\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 73: 100%|██████████| 6/6 [00:04<00:00,  1.29it/s, loss=-0.9831, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 73/150\n",
      "  Train Loss: -0.983075\n",
      "  Val Loss: -0.972849\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 74: 100%|██████████| 6/6 [00:04<00:00,  1.26it/s, loss=-0.9832, sc=0.0070]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch 74/150\n",
      "  Train Loss: -0.983114\n",
      "  Val Loss: -0.972866\n",
      "  Avg dims/word (soft): 10.0 (target: 10)\n",
      "  Avg dims/word (hard): 0.0 (>0.5 threshold)\n",
      "  ✓ New best model!\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 75: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s, loss=-0.9832, sc=0.0070]\n",
      "Exception ignored in: <function _releaseLock at 0x7f0658e95760>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "scaler = GradScaler('cuda') if CONFIG['mixed_precision'] else None\n",
    "\n",
    "# History\n",
    "history = {'train_loss': [], 'val_loss': [], 'avg_relevance_soft': [], 'avg_relevance_hard': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "def compute_avg_relevance(model):\n",
    "    \"\"\"\n",
    "    Compute average dimensions per word.\n",
    "    Returns both soft count (what the loss sees) and hard count (>0.5 threshold).\n",
    "    \"\"\"\n",
    "    rel = torch.sigmoid(model.relevance_logits[:, model.num_anchors:])\n",
    "\n",
    "    # Soft count: sum of sigmoid values (matches loss function)\n",
    "    soft_count = torch.mean(torch.sum(rel, dim=1)).item()\n",
    "\n",
    "    # Hard count: number of dimensions > 0.5 (for interpretability)\n",
    "    hard_count = torch.mean(torch.sum(rel > 0.5, dim=1).float()).item()\n",
    "\n",
    "    return soft_count, hard_count\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING SEMANTIC-DRIVEN TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Key difference: Dimensions emerge through semantic clustering\")\n",
    "print(\"No forced polarity structure - natural semantic axes!\")\n",
    "print(f\"Target: {CONFIG['target_dims_per_word']:.0f} dimensions per word (soft count)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for source_ids, target_ids, target_dists, relations in pbar:\n",
    "        edge_samples = list(zip(source_ids.tolist(), target_ids.tolist(), target_dists.tolist(), relations))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if CONFIG['mixed_precision']:\n",
    "            with autocast('cuda'):\n",
    "                d_loss = distance_loss(model, edge_samples)\n",
    "                sc_loss = semantic_clustering_loss(model, antonym_tensor)\n",
    "                rc_loss = relevance_coherence_loss(model, antonym_tensor)\n",
    "                rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])\n",
    "                commit_loss = relevance_commitment_loss(model)\n",
    "                r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])\n",
    "\n",
    "                total = (\n",
    "                    d_loss +\n",
    "                    CONFIG['semantic_clustering_weight'] * sc_loss +\n",
    "                    CONFIG['relevance_coherence_weight'] * rc_loss +\n",
    "                    CONFIG['relevance_sparsity_weight'] * rs_loss +\n",
    "                    CONFIG['relevance_commitment_weight'] * commit_loss +\n",
    "                    CONFIG['reg_weight'] * r_loss\n",
    "                )\n",
    "            scaler.scale(total).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            d_loss = distance_loss(model, edge_samples)\n",
    "            sc_loss = semantic_clustering_loss(model, antonym_tensor)\n",
    "            rc_loss = relevance_coherence_loss(model, antonym_tensor)\n",
    "            rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])\n",
    "            commit_loss = relevance_commitment_loss(model)\n",
    "            r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])\n",
    "\n",
    "            total = (\n",
    "                d_loss +\n",
    "                CONFIG['semantic_clustering_weight'] * sc_loss +\n",
    "                CONFIG['relevance_coherence_weight'] * rc_loss +\n",
    "                CONFIG['relevance_sparsity_weight'] * rs_loss +\n",
    "                CONFIG['relevance_commitment_weight'] * commit_loss +\n",
    "                CONFIG['reg_weight'] * r_loss\n",
    "            )\n",
    "            total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Preserve anchors\n",
    "        with torch.no_grad():\n",
    "            if model.embeddings.grad is not None:\n",
    "                model.embeddings.grad[:, :CONFIG['num_anchors']] = 0.0\n",
    "            if model.relevance_logits.grad is not None:\n",
    "                model.relevance_logits.grad[:, :CONFIG['num_anchors']] = 0.0\n",
    "\n",
    "        total_loss += total.item()\n",
    "        pbar.set_postfix({'loss': f\"{total.item():.4f}\", 'sc': f\"{sc_loss.item():.4f}\"})\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for source_ids, target_ids, target_dists, relations in val_loader:\n",
    "            edge_samples = list(zip(source_ids.tolist(), target_ids.tolist(), target_dists.tolist(), relations))\n",
    "            d_loss = distance_loss(model, edge_samples)\n",
    "            sc_loss = semantic_clustering_loss(model, antonym_tensor)\n",
    "            rc_loss = relevance_coherence_loss(model, antonym_tensor)\n",
    "            rs_loss = relevance_sparsity_loss(model, CONFIG['target_dims_per_word'])\n",
    "            commit_loss = relevance_commitment_loss(model)\n",
    "            r_loss = regularization_loss(model.embeddings, CONFIG['num_anchors'])\n",
    "\n",
    "            total = (\n",
    "                d_loss +\n",
    "                CONFIG['semantic_clustering_weight'] * sc_loss +\n",
    "                CONFIG['relevance_coherence_weight'] * rc_loss +\n",
    "                CONFIG['relevance_sparsity_weight'] * rs_loss +\n",
    "                CONFIG['relevance_commitment_weight'] * commit_loss +\n",
    "                CONFIG['reg_weight'] * r_loss\n",
    "            )\n",
    "            val_loss += total.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    avg_soft, avg_hard = compute_avg_relevance(model)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['avg_relevance_soft'].append(avg_soft)\n",
    "    history['avg_relevance_hard'].append(avg_hard)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n",
    "    print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"  Avg dims/word (soft): {avg_soft:.1f} (target: {CONFIG['target_dims_per_word']:.0f})\")\n",
    "    print(f\"  Avg dims/word (hard): {avg_hard:.1f} (>0.5 threshold)\")\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        print(f\"  ✓ New best model!\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': CONFIG,\n",
    "        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "    print()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Time: {elapsed/3600:.1f} hours\")\n",
    "print(f\"Best val loss: {best_val_loss:.6f}\")\n",
    "print(f\"Final avg dims/word (soft): {history['avg_relevance_soft'][-1]:.1f}\")\n",
    "print(f\"Final avg dims/word (hard): {history['avg_relevance_hard'][-1]:.1f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icuqYKbMpd0R"
   },
   "source": [
    "## Step 6: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFmIusjWpd0R"
   },
   "outputs": [],
   "source": [
    "# Save embeddings and relevance\n",
    "final_embeddings = model.embeddings.detach().cpu().numpy()\n",
    "final_relevance = torch.sigmoid(model.relevance_logits).detach().cpu().numpy()\n",
    "masked_embeddings = final_embeddings * final_relevance\n",
    "\n",
    "np.save(f\"{CHECKPOINT_DIR}/embeddings.npy\", final_embeddings)\n",
    "np.save(f\"{CHECKPOINT_DIR}/relevance.npy\", final_relevance)\n",
    "np.save(f\"{CHECKPOINT_DIR}/masked_embeddings.npy\", masked_embeddings)\n",
    "\n",
    "# Save vocabulary\n",
    "with open(f\"{CHECKPOINT_DIR}/vocabulary.json\", 'w') as f:\n",
    "    json.dump({'word_to_id': word_to_id, 'id_to_word': {str(k): v for k, v in id_to_word.items()}}, f)\n",
    "\n",
    "# Save history\n",
    "with open(f\"{CHECKPOINT_DIR}/history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Copy to Drive\n",
    "!cp -r {CHECKPOINT_DIR}/* {RESULTS_DIR}/\n",
    "\n",
    "print(f\"✓ Results saved to {RESULTS_DIR}\")\n",
    "print(f\"  - embeddings.npy ({final_embeddings.shape})\")\n",
    "print(f\"  - relevance.npy ({final_relevance.shape})\")\n",
    "print(f\"  - masked_embeddings.npy ({masked_embeddings.shape})\")\n",
    "print(f\"  - vocabulary.json ({len(vocabulary):,} words)\")\n",
    "print(f\"  - history.json\")\n",
    "print(f\"  - best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bj5tsIupd0R"
   },
   "source": [
    "## Step 7: Discover Semantic Axes\n",
    "\n",
    "Analyze which dimensions became semantic axes through natural clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvndWaADpd0R"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DISCOVERING SEMANTIC AXES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get final embeddings and relevance\n",
    "embeddings_np = model.embeddings.detach().cpu().numpy()\n",
    "relevance_np = torch.sigmoid(model.relevance_logits).detach().cpu().numpy()\n",
    "masked_embeddings_np = embeddings_np * relevance_np\n",
    "\n",
    "# Analyze antonym behavior on each dimension\n",
    "idx1 = antonym_tensor[:, 0].cpu().numpy()\n",
    "idx2 = antonym_tensor[:, 1].cpu().numpy()\n",
    "\n",
    "emb1 = masked_embeddings_np[idx1]\n",
    "emb2 = masked_embeddings_np[idx2]\n",
    "diffs = emb1 - emb2\n",
    "\n",
    "# Find dimensions with semantic structure\n",
    "semantic_axes = []\n",
    "for dim in range(CONFIG['num_anchors'], embeddings_np.shape[1]):\n",
    "    dim_diffs = diffs[:, dim]\n",
    "\n",
    "    # Key metrics\n",
    "    mean_abs_diff = np.mean(np.abs(dim_diffs))\n",
    "    sign_consistency = np.abs(np.mean(np.sign(dim_diffs)))\n",
    "\n",
    "    # How many words activate this dimension?\n",
    "    num_active = np.sum(relevance_np[:, dim] > 0.5)\n",
    "\n",
    "    # Semantic axis score\n",
    "    if mean_abs_diff > 0.01 and num_active > 10:\n",
    "        semantic_axes.append({\n",
    "            'dim': dim,\n",
    "            'consistency': sign_consistency,\n",
    "            'mean_diff': mean_abs_diff,\n",
    "            'num_active_words': num_active,\n",
    "            'score': mean_abs_diff * sign_consistency\n",
    "        })\n",
    "\n",
    "semantic_axes.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(semantic_axes)} candidate semantic axes:\\n\")\n",
    "print(f\"{'Dim':<6} {'Consistency':<12} {'Mean Diff':<12} {'Active Words':<15} {'Score':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for axis in semantic_axes[:20]:\n",
    "    print(f\"{axis['dim']:<6} {axis['consistency']:<12.4f} {axis['mean_diff']:<12.4f} \"\n",
    "          f\"{axis['num_active_words']:<15} {axis['score']:<10.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Natural emergence of {len([a for a in semantic_axes if a['consistency'] > 0.2])} strong axes (>20% consistency)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE_PNfgMpd0R"
   },
   "source": [
    "## Step 8: Detailed Semantic Axis Analysis\n",
    "\n",
    "Deep dive into the top semantic axes to see what concepts emerged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUrxqyIhpd0S"
   },
   "outputs": [],
   "source": [
    "# Analyze top 5 semantic axes in detail\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 5 SEMANTIC AXES - DETAILED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for rank, axis in enumerate(semantic_axes[:5], 1):\n",
    "    dim_idx = axis['dim']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Semantic Axis {dim_idx} (Rank #{rank})\")\n",
    "    print(f\"  Consistency: {axis['consistency']:.1%}\")\n",
    "    print(f\"  Active words: {axis['num_active_words']}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Get dimension values (with relevance gating)\n",
    "    dim_values = masked_embeddings_np[:, dim_idx]\n",
    "    dim_relevance = relevance_np[:, dim_idx]\n",
    "\n",
    "    # Show words that activate this dimension\n",
    "    active_words = np.where(dim_relevance > 0.5)[0]\n",
    "    active_values = dim_values[active_words]\n",
    "\n",
    "    sorted_active = active_words[np.argsort(active_values)]\n",
    "\n",
    "    print(\"\\n  POSITIVE POLE (words with high relevance):\")\n",
    "    for idx in sorted_active[-10:][::-1]:\n",
    "        word = id_to_word[idx]\n",
    "        word_display = word.split('_wn.')[0] if '_wn.' in word else word\n",
    "        rel = dim_relevance[idx]\n",
    "        val = dim_values[idx]\n",
    "        print(f\"    {word_display:25s} value={val:+.3f}  relevance={rel:.3f}\")\n",
    "\n",
    "    print(\"\\n  NEGATIVE POLE:\")\n",
    "    for idx in sorted_active[:10]:\n",
    "        word = id_to_word[idx]\n",
    "        word_display = word.split('_wn.')[0] if '_wn.' in word else word\n",
    "        rel = dim_relevance[idx]\n",
    "        val = dim_values[idx]\n",
    "        print(f\"    {word_display:25s} value={val:+.3f}  relevance={rel:.3f}\")\n",
    "\n",
    "    # Show antonym pairs on this axis\n",
    "    print(\"\\n  ANTONYM PAIRS (sample where both have high relevance):\")\n",
    "    shown = 0\n",
    "    for i in range(len(idx1)):\n",
    "        w1_rel = relevance_np[idx1[i], dim_idx]\n",
    "        w2_rel = relevance_np[idx2[i], dim_idx]\n",
    "\n",
    "        if w1_rel > 0.5 and w2_rel > 0.5:  # Both relevant\n",
    "            word1 = id_to_word[idx1[i]]\n",
    "            word2 = id_to_word[idx2[i]]\n",
    "            val1 = masked_embeddings_np[idx1[i], dim_idx]\n",
    "            val2 = masked_embeddings_np[idx2[i], dim_idx]\n",
    "\n",
    "            w1_display = word1.split('_wn.')[0] if '_wn.' in word1 else word1\n",
    "            w2_display = word2.split('_wn.')[0] if '_wn.' in word2 else word2\n",
    "\n",
    "            print(f\"    {w1_display:15s} ({val1:+.3f}) ↔ {w2_display:15s} ({val2:+.3f})\")\n",
    "            shown += 1\n",
    "            if shown >= 10:\n",
    "                break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}