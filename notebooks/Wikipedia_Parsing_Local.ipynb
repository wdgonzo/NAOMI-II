{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAOMI-II Wikipedia Parsing (Local - 10 Workers)\n",
    "## Parse 12.4M Pre-Extracted Sentences Locally\n",
    "\n",
    "**Hardware:** Your local machine with 12 CPU cores (using 10 for safety)\n",
    "\n",
    "**Cost:** FREE! üí∞\n",
    "\n",
    "**Time Estimates (with 10-worker parallelization):**\n",
    "- **100K sentences (test)**: ~12 minutes\n",
    "- **12.4M sentences (full)**: ~29 hours (chart parser)\n",
    "\n",
    "**Why 10 workers not 12?**\n",
    "- Each worker = 1 separate Python process\n",
    "- 10 workers + 1 main notebook process + 1 OS = 12 cores total\n",
    "- Leaves 2 cores free for your system to stay responsive\n",
    "\n",
    "**Data:** Pre-extracted Wikipedia sentences in `notebooks/data/extracted_articles.txt`\n",
    "\n",
    "**Output:**\n",
    "- `data/wikipedia_parsed/parsed_corpus.pkl` (parsed sentences with WSD)\n",
    "- `data/wikipedia_parsed/parse_stats.json` (parsing statistics)\n",
    "\n",
    "**Next Step:** Upload results to Google Drive ‚Üí Run `NAOMI_A100_Training.ipynb` on A100\n",
    "\n",
    "**Key Advantage:** Chart parser evaluates ALL parse options (most robust for Wikipedia's complex sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Local Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì NAOMI-II directory detected\n",
      "\n",
      "CPU cores: 12\n",
      "‚úì 12+ cores available - Excellent for parallel parsing!\n",
      "  Expected speed: ~144 sentences/sec (chart parser)\n",
      "  Est. time for 12.4M sentences: ~24 hours\n",
      "\n",
      "RAM: 16.8 GB\n",
      "‚úì Sufficient RAM for parsing\n",
      "\n",
      "Disk space free: 220.4 GB\n",
      "‚úì Sufficient disk space for parsed output\n",
      "\n",
      "Checking dependencies...\n",
      "‚úì All dependencies installed\n",
      "‚úì WordNet data available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify we're in the NAOMI-II directory\n",
    "if not Path('../scripts/batch_parse_corpus.py').exists():\n",
    "    print(\"‚ö†Ô∏è ERROR: Not in NAOMI-II directory!\")\n",
    "    print(\"Please run this notebook from the NAOMI-II/notebooks/ directory\")\n",
    "else:\n",
    "    print(\"‚úì NAOMI-II directory detected\")\n",
    "\n",
    "# Check CPU cores\n",
    "cpu_count = mp.cpu_count()\n",
    "print(f\"\\nCPU cores: {cpu_count}\")\n",
    "if cpu_count >= 12:\n",
    "    print(\"‚úì 12+ cores available - Excellent for parallel parsing!\")\n",
    "    print(f\"  Expected speed: ~{cpu_count * 12} sentences/sec (chart parser)\")\n",
    "    est_hours = 12400000 / (cpu_count * 12 * 3600)\n",
    "    print(f\"  Est. time for 12.4M sentences: ~{est_hours:.0f} hours\")\n",
    "elif cpu_count >= 8:\n",
    "    print(f\"‚úì {cpu_count} cores available - Good for parallel parsing\")\n",
    "    print(f\"  Expected speed: ~{cpu_count * 12} sentences/sec (chart parser)\")\n",
    "    est_hours = 12400000 / (cpu_count * 12 * 3600)\n",
    "    print(f\"  Est. time for 12.4M sentences: ~{est_hours:.0f} hours\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Only {cpu_count} cores - parsing will be slower\")\n",
    "    est_hours = 12400000 / (cpu_count * 12 * 3600)\n",
    "    print(f\"  Est. time for 12.4M sentences: ~{est_hours:.0f} hours\")\n",
    "\n",
    "# Check RAM\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print(f\"\\nRAM: {ram_gb:.1f} GB\")\n",
    "if ram_gb >= 16:\n",
    "    print(\"‚úì Sufficient RAM for parsing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Low RAM - consider reducing batch size if errors occur\")\n",
    "\n",
    "# Check disk space\n",
    "disk = psutil.disk_usage('.')\n",
    "disk_free_gb = disk.free / 1e9\n",
    "print(f\"\\nDisk space free: {disk_free_gb:.1f} GB\")\n",
    "if disk_free_gb >= 20:\n",
    "    print(\"‚úì Sufficient disk space for parsed output\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Low disk space - may need to clean up\")\n",
    "\n",
    "# Check dependencies\n",
    "print(\"\\nChecking dependencies...\")\n",
    "try:\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    import tqdm\n",
    "    print(\"‚úì All dependencies installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Missing dependency: {e}\")\n",
    "    print(\"   Run: pip install nltk numpy tqdm\")\n",
    "\n",
    "# Check WordNet\n",
    "try:\n",
    "    from nltk.corpus import wordnet\n",
    "    wordnet.synsets('test')\n",
    "    print(\"‚úì WordNet data available\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è WordNet not downloaded\")\n",
    "    print(\"   Run: python -c \\\"import nltk; nltk.download('wordnet'); nltk.download('omw-1.4')\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Pre-extracted sentences found!\n",
      "  File size: 1365 MB\n",
      "\n",
      "Counting sentences...\n",
      "  Total sentences: 12,377,687\n",
      "\n",
      "Sample sentences:\n",
      "  1. A historically left-wing movement, anarchism is usually described as the libertarian wing of the soc...\n",
      "  2. Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from t...\n",
      "  3. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement...\n",
      "\n",
      "Estimated parsing time (12 cores):\n",
      "  Chart parser (robust):   ~24 hours\n",
      "  Quantum parser (faster): ~17 hours\n",
      "\n",
      "  Using chart parser for maximum robustness on Wikipedia text\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check for extracted sentences\n",
    "data_file = Path('data/extracted_articles.txt')\n",
    "\n",
    "if data_file.exists():\n",
    "    print(\"‚úì Pre-extracted sentences found!\")\n",
    "    \n",
    "    # Get file size\n",
    "    size_mb = data_file.stat().st_size / 1e6\n",
    "    print(f\"  File size: {size_mb:.0f} MB\")\n",
    "    \n",
    "    # Count sentences (quick estimate)\n",
    "    print(\"\\nCounting sentences...\")\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        sentence_count = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"  Total sentences: {sentence_count:,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample sentences:\")\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 3:\n",
    "                print(f\"  {i+1}. {line.strip()[:100]}...\")\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # Time estimates\n",
    "    cpu_count = mp.cpu_count()\n",
    "    est_hours_chart = sentence_count / (cpu_count * 12 * 3600)\n",
    "    est_hours_quantum = sentence_count / (cpu_count * 17 * 3600)\n",
    "    \n",
    "    print(f\"\\nEstimated parsing time ({cpu_count} cores):\")\n",
    "    print(f\"  Chart parser (robust):   ~{est_hours_chart:.0f} hours\")\n",
    "    print(f\"  Quantum parser (faster): ~{est_hours_quantum:.0f} hours\")\n",
    "    print(f\"\\n  Using chart parser for maximum robustness on Wikipedia text\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ERROR: Data file not found!\")\n",
    "    print(f\"  Expected location: {data_file.absolute()}\")\n",
    "    print(\"\\nPlease ensure extracted_articles.txt is in notebooks/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Parse (100K sentences - RECOMMENDED)\n",
    "\n",
    "**‚ö†Ô∏è START HERE BEFORE FULL PARSE**\n",
    "\n",
    "**Time:** ~10 minutes with 12 cores\n",
    "\n",
    "**Purpose:** Verify everything works and check parse success rate before committing to 24-hour run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores: 12\n",
      "Using 6 worker processes (Windows-safe configuration)\n",
      "This creates 6 Python processes + 1 main notebook process = 7 total\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test parse with 100K sentences\n",
    "# Using CHART parser (evaluates all options - most robust)\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# WINDOWS FIX: Use fewer workers due to Windows multiprocessing overhead\n",
    "# Start with 6 workers, can adjust up/down based on system performance\n",
    "num_workers = 6  # Conservative for Windows\n",
    "print(f\"Total CPU cores: {mp.cpu_count()}\")\n",
    "print(f\"Using {num_workers} worker processes (Windows-safe configuration)\")\n",
    "print(f\"This creates {num_workers} Python processes + 1 main notebook process = {num_workers + 1} total\\n\")\n",
    "\n",
    "!python ../scripts/batch_parse_corpus.py \\\n",
    "    --corpus data/extracted_articles.txt \\\n",
    "    --parser-type chart \\\n",
    "    --max-sentences 100000 \\\n",
    "    --batch-size 1000 \\\n",
    "    --checkpoint-every 10000 \\\n",
    "    --num-workers {num_workers} \\\n",
    "    --output-dir ../data/wikipedia_parsed_100k \\\n",
    "    --resume\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST PARSE COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test parse statistics\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_file = Path('../data/wikipedia_parsed_100k/parse_stats.json')\n",
    "\n",
    "if stats_file.exists():\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"Test Parse Statistics:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total sentences: {stats.get('total', 0):,}\")\n",
    "    print(f\"Successful parses: {stats.get('success', 0):,}\")\n",
    "    print(f\"Failed parses: {stats.get('failed', 0):,}\")\n",
    "    \n",
    "    success_rate = stats.get('success', 0) / max(stats.get('total', 1), 1) * 100\n",
    "    print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average parse score: {stats.get('avg_score', 0):.3f}\")\n",
    "    print(f\"Total triples extracted: {stats.get('total_triples', 0):,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(\"=\"*70)\n",
    "    if success_rate >= 90:\n",
    "        print(\"‚úì Excellent success rate (>90%)!\")\n",
    "        print(\"  You can proceed with full parse using chart parser\")\n",
    "        print(\"  Or try quantum parser for faster parsing (Cell 4b)\")\n",
    "    elif success_rate >= 80:\n",
    "        print(\"‚úì Good success rate (80-90%)\")\n",
    "        print(\"  Proceed with chart parser for robustness\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Low success rate (<80%)\")\n",
    "        print(\"  Chart parser is already the most robust option\")\n",
    "        print(\"  Check error log for common failure patterns\")\n",
    "    \n",
    "else:\n",
    "    print(\"No statistics file found - did the test parse complete?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Full Parse with Chart Parser (RECOMMENDED)\n",
    "\n",
    "**Parser:** Chart (most robust, evaluates ALL parse options)\n",
    "\n",
    "**Time:** ~24 hours for 12.4M sentences (with 12 cores)\n",
    "\n",
    "**Cost:** FREE (runs locally)\n",
    "\n",
    "**Checkpointing:** Saves every 50K sentences (fully resumable)\n",
    "\n",
    "**‚ö†Ô∏è This will run for ~24 hours - let it run overnight/weekend!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Full parse with chart parser (MOST ROBUST)\n",
    "# With 10-worker parallelization: ~120 sent/sec\n",
    "# This will take ~29 hours\n",
    "# Checkpoints every 50K sentences (resumable with --resume flag)\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Use 10 workers (leave 2 cores free for system)\n",
    "num_workers = max(1, mp.cpu_count() - 2)\n",
    "print(f\"Total CPU cores: {mp.cpu_count()}\")\n",
    "print(f\"Using {num_workers} worker processes (leaving 2 cores free)\")\n",
    "print(f\"Expected speed: ~{num_workers * 12} sentences/sec\")\n",
    "print(f\"Expected time: ~{12400000 / (num_workers * 12 * 3600):.0f} hours\\n\")\n",
    "\n",
    "!python ../scripts/batch_parse_corpus.py \\\n",
    "    --corpus data/extracted_articles.txt \\\n",
    "    --parser-type chart \\\n",
    "    --max-sentences 12400000 \\\n",
    "    --batch-size 1000 \\\n",
    "    --checkpoint-every 50000 \\\n",
    "    --num-workers {num_workers} \\\n",
    "    --output-dir ../data/wikipedia_parsed \\\n",
    "    --resume\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARSING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Full Parse with Quantum Parser (ALTERNATIVE - Faster)\n",
    "\n",
    "**Parser:** Quantum (faster, smart branching)\n",
    "\n",
    "**Time:** ~17 hours for 12.4M sentences (with 12 cores)\n",
    "\n",
    "**Use only if:** Test parse showed >90% success rate with quantum parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Full parse with quantum parser (FASTER)\n",
    "# With 10-worker parallelization: ~170 sent/sec\n",
    "# This will take ~20 hours\n",
    "# Checkpoints every 50K sentences (resumable)\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Use 10 workers (leave 2 cores free for system)\n",
    "num_workers = max(1, mp.cpu_count() - 2)\n",
    "print(f\"Total CPU cores: {mp.cpu_count()}\")\n",
    "print(f\"Using {num_workers} worker processes (leaving 2 cores free)\")\n",
    "print(f\"Expected speed: ~{num_workers * 17} sentences/sec\")\n",
    "print(f\"Expected time: ~{12400000 / (num_workers * 17 * 3600):.0f} hours\\n\")\n",
    "\n",
    "!python ../scripts/batch_parse_corpus.py \\\n",
    "    --corpus data/extracted_articles.txt \\\n",
    "    --parser-type quantum \\\n",
    "    --max-sentences 12400000 \\\n",
    "    --batch-size 1000 \\\n",
    "    --checkpoint-every 50000 \\\n",
    "    --num-workers {num_workers} \\\n",
    "    --output-dir ../data/wikipedia_parsed \\\n",
    "    --resume\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PARSING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Final Parse Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check for full parse stats\n",
    "stats_file = Path('../data/wikipedia_parsed/parse_stats.json')\n",
    "\n",
    "if not stats_file.exists():\n",
    "    # Fall back to test parse\n",
    "    stats_file = Path('../data/wikipedia_parsed_100k/parse_stats.json')\n",
    "\n",
    "if stats_file.exists():\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"Final Parse Statistics:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total sentences: {stats.get('total', 0):,}\")\n",
    "    print(f\"Successful parses: {stats.get('success', 0):,}\")\n",
    "    print(f\"Failed parses: {stats.get('failed', 0):,}\")\n",
    "    \n",
    "    success_rate = stats.get('success', 0) / max(stats.get('total', 1), 1) * 100\n",
    "    print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average parse score: {stats.get('avg_score', 0):.3f}\")\n",
    "    print(f\"Total triples extracted: {stats.get('total_triples', 0):,}\")\n",
    "    \n",
    "    avg_triples = stats.get('total_triples', 0) / max(stats.get('success', 1), 1)\n",
    "    print(f\"Average triples per sentence: {avg_triples:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Show output files\n",
    "    output_dir = Path('../data/wikipedia_parsed')\n",
    "    if output_dir.exists():\n",
    "        print(\"\\nOutput files:\")\n",
    "        for file in output_dir.glob('*'):\n",
    "            size_mb = file.stat().st_size / 1e6\n",
    "            print(f\"  {file.name}: {size_mb:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"No statistics file found - has parsing been run?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps: Upload to Google Drive for A100 Training\n",
    "\n",
    "**You have successfully parsed Wikipedia locally! üéâ**\n",
    "\n",
    "### Upload to Google Drive:\n",
    "\n",
    "1. **Open Google Drive** in your browser\n",
    "2. **Navigate to:** `My Drive/NAOMI-II-data/`\n",
    "3. **Upload these files:**\n",
    "   - `data/wikipedia_parsed/parsed_corpus.pkl` (large file, ~10-20GB)\n",
    "   - `data/wikipedia_parsed/parse_stats.json` (small)\n",
    "\n",
    "4. **Rename in Drive:**\n",
    "   - `parsed_corpus.pkl` ‚Üí `wikipedia_parsed_corpus.pkl`\n",
    "   - `parse_stats.json` ‚Üí `wikipedia_parse_stats.json`\n",
    "\n",
    "### Then run A100 training:\n",
    "\n",
    "1. Open `colab-results/NAOMI_A100_Training.ipynb` in Google Colab\n",
    "2. Switch to **A100 GPU** runtime (7 credits/hour)\n",
    "3. Run the training notebook (~6 hours, $4.50)\n",
    "\n",
    "### Total Project Cost:\n",
    "- **Parsing (local)**: $0.00 ‚úì\n",
    "- **Training (A100)**: $4.50\n",
    "- **TOTAL**: **$4.50** (vs $33 if you had parsed on Colab!)\n",
    "\n",
    "---\n",
    "\n",
    "**Alternative: Use command line to upload to Drive**\n",
    "\n",
    "If you have Google Drive desktop app installed, you can just copy the files:\n",
    "\n",
    "```bash\n",
    "# Copy to Google Drive (adjust path for your system)\n",
    "cp data/wikipedia_parsed/parsed_corpus.pkl \"G:\\My Drive\\NAOMI-II-data\\wikipedia_parsed_corpus.pkl\"\n",
    "cp data/wikipedia_parsed/parse_stats.json \"G:\\My Drive\\NAOMI-II-data\\wikipedia_parse_stats.json\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
