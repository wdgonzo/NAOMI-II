{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# NAOMI-II Training on Google Colab\n",
    "\n",
    "This notebook trains NAOMI-II embeddings on the full WordNet dataset (157K vocabulary, 15.67M edges).\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro (free for students)\n",
    "- T4 GPU runtime\n",
    "- Pre-generated training data uploaded to Google Drive\n",
    "\n",
    "**Expected Runtime:** 12-24 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_gpu"
   },
   "source": [
    "## 1. Setup GPU Runtime\n",
    "\n",
    "**IMPORTANT:** Before running, change runtime type:\n",
    "1. Runtime → Change runtime type\n",
    "2. Hardware accelerator: **GPU**\n",
    "3. GPU type: **T4** (or best available)\n",
    "4. Click **Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! Change runtime type to GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "git_clone"
   },
   "outputs": [],
   "source": [
    "# Clone NAOMI-II repository\n",
    "!git clone https://github.com/YOUR_USERNAME/NAOMI-II.git\n",
    "%cd NAOMI-II\n",
    "\n",
    "# Show current directory\n",
    "!pwd\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_deps"
   },
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q nltk numpy tqdm\n",
    "\n",
    "# Download WordNet\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 4. Mount Google Drive and Load Data\n",
    "\n",
    "**Before running:** Upload your pre-generated data to Google Drive:\n",
    "- `full_wordnet/` folder (extracted WordNet synsets)\n",
    "- `wordnet_training/` folder (15.67M training edges)\n",
    "\n",
    "Suggested location: `My Drive/NAOMI-II-data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# List available data (adjust path to your Drive location)\n",
    "!ls -lh /content/drive/MyDrive/NAOMI-II-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_data"
   },
   "outputs": [],
   "source": [
    "# Copy data from Drive to local Colab storage (faster for training)\n",
    "import os\n",
    "\n",
    "# Adjust this path to match your Google Drive structure\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/NAOMI-II-data\"\n",
    "\n",
    "# Create data directories\n",
    "!mkdir -p data/full_wordnet\n",
    "!mkdir -p data/wordnet_training\n",
    "\n",
    "# Copy data (this may take a few minutes)\n",
    "print(\"Copying WordNet data...\")\n",
    "!cp -r {DRIVE_DATA_PATH}/full_wordnet/* data/full_wordnet/\n",
    "print(\"Copying training data...\")\n",
    "!cp -r {DRIVE_DATA_PATH}/wordnet_training/* data/wordnet_training/\n",
    "\n",
    "# Verify data copied correctly\n",
    "print(\"\\n✓ Data copied successfully:\")\n",
    "!ls -lh data/full_wordnet/\n",
    "!ls -lh data/wordnet_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alternative_data"
   },
   "source": [
    "### Alternative: Generate Data (if not pre-uploaded)\n",
    "\n",
    "**Skip this if you already copied data above.**\n",
    "\n",
    "If you didn't upload pre-generated data, you can generate it here (~1.5 hours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_data"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Only run if you need to generate data from scratch\n",
    "\n",
    "# Extract WordNet (~30 minutes)\n",
    "# !python scripts/extract_full_wordnet.py --output data/full_wordnet\n",
    "\n",
    "# Generate training edges (~1 hour)\n",
    "# !python scripts/generate_wordnet_training_data.py \\\n",
    "#     --input data/full_wordnet \\\n",
    "#     --output data/wordnet_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Start Training\n",
    "\n",
    "This will train for 50 epochs on 15.67M edges with dynamic dimension expansion.\n",
    "\n",
    "**Expected time:** 12-24 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python scripts/train_embeddings.py \\\n",
    "    --training-data data/wordnet_training \\\n",
    "    --unsupervised \\\n",
    "    --dynamic-dims \\\n",
    "    --embedding-dim 128 \\\n",
    "    --max-dims 512 \\\n",
    "    --epochs 50 \\\n",
    "    --lr 0.001 \\\n",
    "    --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor"
   },
   "source": [
    "## 6. Monitor Training Progress\n",
    "\n",
    "The training output will show:\n",
    "- Loss trends (distance + sparsity)\n",
    "- Dimension statistics every 10 epochs\n",
    "- Dimension expansion events\n",
    "\n",
    "**Expected behavior:**\n",
    "- Initial loss: ~0.08\n",
    "- Final loss: ~0.01-0.02\n",
    "- Dimensions: May expand from 128 → 192-256 if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "checkpoints"
   },
   "source": [
    "## 7. Download Trained Model\n",
    "\n",
    "After training completes, download checkpoints back to your Google Drive or local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_checkpoints"
   },
   "outputs": [],
   "source": [
    "# Copy checkpoints to Google Drive for safekeeping\n",
    "!mkdir -p /content/drive/MyDrive/NAOMI-II-results\n",
    "!cp -r data/checkpoints /content/drive/MyDrive/NAOMI-II-results/\n",
    "\n",
    "print(\"✓ Checkpoints saved to Google Drive\")\n",
    "!ls -lh /content/drive/MyDrive/NAOMI-II-results/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_local"
   },
   "outputs": [],
   "source": [
    "# Optional: Download directly to your computer\n",
    "from google.colab import files\n",
    "\n",
    "# Download final embeddings\n",
    "files.download('data/checkpoints/best_model.pkl')\n",
    "files.download('data/checkpoints/embeddings_final.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis"
   },
   "source": [
    "## 8. Quick Analysis (Optional)\n",
    "\n",
    "Run basic analysis on the trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze"
   },
   "outputs": [],
   "source": [
    "# Load trained embeddings\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "embeddings = np.load('data/checkpoints/embeddings_final.npy')\n",
    "with open('data/wordnet_training/vocabulary.json', 'r') as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocab_data['word_to_id'])}\")\n",
    "print(f\"\\nDimension statistics:\")\n",
    "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings.std():.4f}\")\n",
    "print(f\"  Min: {embeddings.min():.4f}\")\n",
    "print(f\"  Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Compute sparsity\n",
    "sparsity = np.mean(np.abs(embeddings) < 0.001)\n",
    "print(f\"\\nSparsity: {sparsity:.1%} of values near zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## 9. Cleanup (Optional)\n",
    "\n",
    "Free up Colab storage after saving checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_code"
   },
   "outputs": [],
   "source": [
    "# Remove large data files to free up space\n",
    "# !rm -rf data/full_wordnet\n",
    "# !rm -rf data/wordnet_training\n",
    "\n",
    "# Check remaining disk space\n",
    "!df -h /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes\n",
    "\n",
    "**Session Management:**\n",
    "- Colab Pro sessions can run for 24 hours\n",
    "- Training should complete in 12-24 hours on T4\n",
    "- Checkpoints are saved every 5 epochs automatically\n",
    "\n",
    "**If Session Disconnects:**\n",
    "1. Reconnect to the same runtime\n",
    "2. Re-mount Google Drive\n",
    "3. Resume from last checkpoint (modify training command)\n",
    "\n",
    "**Troubleshooting:**\n",
    "- Out of memory: Reduce `--batch-size` to 64 or 32\n",
    "- Slow training: Verify GPU is enabled (check cell 1)\n",
    "- Data not found: Check Google Drive paths in cell 4\n",
    "\n",
    "**After Training:**\n",
    "- Checkpoints saved to Google Drive\n",
    "- Download to local machine for analysis\n",
    "- Run dimension discovery analysis locally on Surface"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
